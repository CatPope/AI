{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bff9e7-7afa-4ef6-bd9f-465fb147fe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = random.choice([\"안녕하세요\", \"사랑합니다\", \"배가 고파요\"])\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acff5f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* Running on public URL: https://3c7dc0bc787d042909.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3c7dc0bc787d042909.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7872 <> https://3c7dc0bc787d042909.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 가져오기\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API Key가 설정되지 않았습니다. 환경 변수 'OPENAI_API_KEY'를 설정하세요.\")\n",
    "\n",
    "# 답변 생성 함수\n",
    "def answer(state, state_chatbot, text):\n",
    "    # 이전 대화 기록에 사용자 메시지 추가\n",
    "    messages = state + [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "    # ChatGPT API 호출\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # 사용 가능한 모델 이름 확인\n",
    "            messages=messages\n",
    "        )\n",
    "        # ChatGPT의 응답 추출\n",
    "        msg = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # 새로운 대화 기록 추가\n",
    "        new_state = [{\"role\": \"user\", \"content\": text}, {\"role\": \"assistant\", \"content\": msg}]\n",
    "        state = state + new_state\n",
    "        state_chatbot = state_chatbot + [(text, msg)]\n",
    "\n",
    "        return state, state_chatbot, state_chatbot\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 응답\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        state_chatbot.append((text, error_msg))\n",
    "        return state, state_chatbot, state_chatbot\n",
    "\n",
    "\n",
    "# Gradio UI 생성\n",
    "with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:750px}\") as demo:\n",
    "    # 대화 상태 저장\n",
    "    state = gr.State([{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}])\n",
    "    state_chatbot = gr.State([])\n",
    "\n",
    "    # 헤더 UI\n",
    "    with gr.Row():\n",
    "        gr.HTML(\"\"\"<div style=\"text-align: center; max-width: 500px; margin: 0 auto;\">\n",
    "            <div>\n",
    "                <h1>Jaemin's ChatGPT</h1>\n",
    "            </div>\n",
    "        </div>\"\"\")\n",
    "\n",
    "    # 채팅 UI\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(elem_id=\"chatbot\")  # 채팅 인터페이스\n",
    "\n",
    "    # 텍스트 입력 UI\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Send a message...\")  # .style() 메서드 제거\n",
    "\n",
    "    # 이벤트 핸들링: 텍스트 제출 시\n",
    "    txt.submit(answer, [state, state_chatbot, txt], [state, state_chatbot, chatbot])\n",
    "    txt.submit(lambda: \"\", None, txt)  # 텍스트 박스 초기화\n",
    "\n",
    "# Gradio 애플리케이션 실행\n",
    "demo.launch(debug=True, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acce878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 2042, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1589, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_45103/1746165132.py\", line 37, in respond\n",
      "    chat_history[-1] = (chat_history[-1][0], full_response)\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox ()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def user(message, chat_history):\n",
    "        chat_history.append((message, \"\"))\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    def respond(message, chat_history):\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "        for user_msg, bot_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if bot_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, \"content\") and delta.content:\n",
    "                full_response += delta.content\n",
    "\n",
    "        chat_history[-1] = (chat_history[-1][0], full_response)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2ed98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7877\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 2052, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1808, in postprocess_data\n",
      "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1763, in validate_outputs\n",
      "    raise ValueError(\n",
      "ValueError: A  function (chatbot_interface) didn't return enough output values (needed: 2, returned: 1).\n",
      "    Output components:\n",
      "        [chatbot, state]\n",
      "    Output values returned:\n",
      "        [['테스트', 'Error: \\n\\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\\n\\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \\n\\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\\n\\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\\n']]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 2052, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1858, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 596, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "  File \"/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 395, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ChatGPT와 스트리밍 대화를 처리하는 함수\n",
    "def stream_chat(messages):\n",
    "    \"\"\"\n",
    "    ChatGPT API와 통신하며 스트리밍 방식으로 응답을 반환하는 함수\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # OpenAI API 호출\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            stream=True  # 스트리밍 활성화\n",
    "        )\n",
    "        bot_message = \"\"\n",
    "        # 스트리밍 응답 처리\n",
    "        for chunk in response:\n",
    "            if \"choices\" in chunk:\n",
    "                delta = chunk[\"choices\"][0][\"delta\"]\n",
    "                if \"content\" in delta:\n",
    "                    bot_message += delta[\"content\"]\n",
    "                    yield bot_message  # 실시간으로 업데이트\n",
    "    except Exception as e:\n",
    "        yield f\"Error: {e}\"\n",
    "\n",
    "# Gradio에서 사용하는 함수\n",
    "def chatbot_interface(user_message, chat_history):\n",
    "    \"\"\"\n",
    "    Gradio 인터페이스에서 입력 메시지를 받아 ChatGPT와의 대화를 관리\n",
    "    \"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # OpenAI API에 전달할 메시지 포맷 생성\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for user, bot in chat_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "        if bot is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot})\n",
    "\n",
    "    # 사용자 메시지를 추가\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    chat_history.append([user_message, None])  # Gradio Chatbot에서 요구하는 형식\n",
    "\n",
    "    # 스트리밍 방식으로 응답 처리\n",
    "    bot_response = \"\"\n",
    "    for chunk in stream_chat(messages):\n",
    "        bot_response = chunk\n",
    "        chat_history[-1][1] = bot_response  # Gradio 형식에 맞게 실시간으로 업데이트\n",
    "        yield chat_history  # 실시간으로 Gradio 인터페이스에 반영\n",
    "\n",
    "    # 최종 응답 저장\n",
    "    chat_history[-1][1] = bot_response\n",
    "    return chat_history\n",
    "\n",
    "# Gradio UI 구성\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ChatGPT with Streaming\")  # Chatbot 인터페이스\n",
    "    user_input = gr.Textbox(label=\"Type your message:\")  # 사용자 입력\n",
    "    clear_btn = gr.Button(\"Clear Chat\")  # 채팅 기록 초기화 버튼\n",
    "\n",
    "    # 대화 기록을 상태로 관리\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 메시지 입력 시 대화 업데이트\n",
    "    user_input.submit(chatbot_interface, inputs=[user_input, state], outputs=[chatbot, state])\n",
    "    clear_btn.click(lambda: ([], []), inputs=None, outputs=[chatbot, state])  # 초기화 버튼\n",
    "\n",
    "# Gradio 애플리케이션 실행\n",
    "demo.queue()\n",
    "demo.launch(server_name='0.0.0.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat = Chat()\n",
    "\n",
    "def add_text(state, text):\n",
    "    chat.query(text)\n",
    "    result = chat.messages[-1]['content']\n",
    "    state = state + [(text, result)]\n",
    "    return state, state\n",
    "\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n",
    "\n",
    "    txt.submit(add_text, [state, txt], [state, chatbot])\n",
    "    txt.submit(lambda :\"\", None, txt)\n",
    "\n",
    "demo.launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a053cd-041e-462a-9920-f660e4803b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스트림\n",
    "\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        bot_message = random.choice([\"안녕하세요\", \"사랑합니다\", \"배가 고파요\"])\n",
    "        history[-1][1] = \"\"\n",
    "        for character in bot_message:\n",
    "            history[-1][1] += character\n",
    "            time.sleep(0.05)\n",
    "            yield history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "demo.queue()\n",
    "demo.launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8774ca2e-f0e1-4944-94f4-a1bd177a88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 출력에 대해서 싫어요를 눌렀습니다. : 안녕하세요 안녕하세요씨\n",
      "다음 출력에 대해서 좋아요를 눌렀습니다. : 안녕하세요 안녕하세요씨\n",
      "다음 출력에 대해서 좋아요를 눌렀습니다. : 안녕하세요 반갑습니다.씨\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(history, input):\n",
    "    return history + [(input, \"안녕하세요 \" + input + \"씨\")]\n",
    "\n",
    "def vote(data: gr.LikeData):\n",
    "    if data.liked:\n",
    "        print(\"다음 출력에 대해서 좋아요를 눌렀습니다. : \" + data.value)\n",
    "    else:\n",
    "        print(\"다음 출력에 대해서 싫어요를 눌렀습니다. : \" + data.value)\n",
    "    \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    textbox = gr.Textbox()\n",
    "    textbox.submit(greet, [chatbot, textbox], [chatbot])\n",
    "    chatbot.like(vote, None, None)  # Adding this line causes the like/dislike icons to appear in your chatbot\n",
    "    \n",
    "demo.launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34d7f31-ae58-4690-b2df-60ec5e3a85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmjung/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, gr.update(value=\"\", interactive=False)\n",
    "\n",
    "def add_file(history, file):\n",
    "    history = history + [((file.name,), None)]\n",
    "    return history\n",
    "\n",
    "def bot(history):\n",
    "    response = \"멋져요!\"\n",
    "    history[-1][1] = \"\"\n",
    "    for character in response:\n",
    "        history[-1][1] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(\"avatar.png\"))),\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            scale=4,\n",
    "            show_label=False,\n",
    "            placeholder=\"텍스트를 입력하고 엔터를 치거나 이미지를 업로드하세요\",\n",
    "            container=False,\n",
    "        )\n",
    "        btn = gr.UploadButton(\"Upload\", file_types=[\"image\", \"video\", \"audio\"])\n",
    "\n",
    "    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)\n",
    "    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(server_name='0.0.0.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11263ba-5498-4fa1-92d5-5c6f450e67b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
