{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xTTS-2.v fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로깅 파라미터\n",
    "RUN_NAME = \"GPT_XTTS_v2.0_BBANGHYONG_FT\"\n",
    "PROJECT_NAME = \"XTTS_trainer\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "# 체크포인트가 저장될 경로를 설정합니다. 기본값: ./run/training/\n",
    "OUT_PATH = os.path.join(os.getcwd(), \"run\", \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 파라미터\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # 멀티 GPU 훈련의 경우 False로 설정하세요\n",
    "START_WITH_EVAL = True  # True로 설정하면 평가부터 시작합니다\n",
    "BATCH_SIZE = 3  # 배치 크기를 설정하세요\n",
    "GRAD_ACUMM_STEPS = 84  # 그래디언트 누적 단계를 설정하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝에 사용할 데이터셋을 여기에 정의하세요.\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    dataset_name=\"bbanghyong\",\n",
    "    path=\"./wavs\",\n",
    "    meta_file_train=\"../metadata.txt\",\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "# 데이터셋의 설정을 여기에 추가하세요\n",
    "DATASETS_CONFIG_LIST = [config_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > DVAE 파일을 다운로드합니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.07k/1.07k [00:00<00:00, 1.72kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > XTTS v2.0 파일을 다운로드합니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211M/211M [00:03<00:00, 53.3MiB/s]\n",
      "100%|██████████| 361k/361k [00:01<00:00, 346kiB/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# XTTS v2.0.1 파일이 다운로드될 경로를 정의하세요\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE 파일들\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# 다운로드된 파일의 경로를 설정하세요\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE 파일이 필요한 경우 다운로드합니다\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > DVAE 파일을 다운로드합니다!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "# 필요한 경우 XTTS v2.0 체크포인트를 다운로드합니다\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# XTTS 전이 학습 파라미터: 파인튜닝할 XTTS 모델 체크포인트의 경로를 제공해야 합니다.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json 파일\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth 파일\n",
    "\n",
    "# 필요한 경우 XTTS v2.0 파일을 다운로드합니다\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > XTTS v2.0 파일을 다운로드합니다!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: d:\\GitHub\\AI\\LangChain\\xTTS\\run\\training\\XTTS_v2.0_original_model_files/dvae.pth\n",
      " | > Found 332 files in D:\\GitHub\\AI\\LangChain\\xTTS\\wavs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Num. of CPUs: 8\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=d:\\GitHub\\AI\\LangChain\\xTTS\\run\\training\\GPT_XTTS_v2.0_BBANGHYONG_FT-June-24-2024_11+36PM-ad82aa3f\n",
      "\n",
      " > Model has 518442047 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/1000\u001b[0m\n",
      " --> d:\\GitHub\\AI\\LangChain\\xTTS\\run\\training\\GPT_XTTS_v2.0_BBANGHYONG_FT-June-24-2024_11+36PM-ad82aa3f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Filtering invalid eval samples!!\n",
      " > Total eval samples after filtering: 0\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: 'd:/GitHub/AI/LangChain/xTTS/run/training/GPT_XTTS_v2.0_BBANGHYONG_FT-June-24-2024_11+36PM-ad82aa3f\\\\trainer_0_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:1833\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1832\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit()\n\u001b[0;32m   1834\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mrank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mrun_eval:\n\u001b[1;32m-> 1787\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_epoch()\n\u001b[0;32m   1788\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtest_delay_epochs \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mrank \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:1628\u001b[0m, in \u001b[0;36mTrainer.eval_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_loader \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_loader \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_eval_dataloader(\n\u001b[0;32m   1629\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_assets,\n\u001b[0;32m   1630\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_samples,\n\u001b[0;32m   1631\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1632\u001b[0m         )\n\u001b[0;32m   1633\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mrun_eval\n\u001b[0;32m   1634\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m     )\n\u001b[0;32m   1637\u001b[0m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[1;34m(self, training_assets, samples, verbose)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[39mreturn\u001b[39;00m loader\n\u001b[1;32m--> 990\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_loader(\n\u001b[0;32m    991\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    992\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    993\u001b[0m     training_assets,\n\u001b[0;32m    994\u001b[0m     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    995\u001b[0m     samples,\n\u001b[0;32m    996\u001b[0m     verbose,\n\u001b[0;32m    997\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_gpus,\n\u001b[0;32m    998\u001b[0m )\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:914\u001b[0m, in \u001b[0;36mTrainer._get_loader\u001b[1;34m(self, model, config, assets, is_eval, samples, verbose, num_gpus)\u001b[0m\n\u001b[0;32m    909\u001b[0m         loader \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_data_loader(\n\u001b[0;32m    910\u001b[0m             config\u001b[39m=\u001b[39mconfig, assets\u001b[39m=\u001b[39massets, is_eval\u001b[39m=\u001b[39mis_eval, samples\u001b[39m=\u001b[39msamples, verbose\u001b[39m=\u001b[39mverbose, num_gpus\u001b[39m=\u001b[39mnum_gpus\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    913\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m--> 914\u001b[0m     \u001b[39mlen\u001b[39m(loader) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    915\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39m ❗ len(DataLoader) returns 0. Make sure your dataset is not empty or len(dataset) > 0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m loader\n",
      "\u001b[1;31mAssertionError\u001b[0m:  ❗ len(DataLoader) returns 0. Make sure your dataset is not empty or len(dataset) > 0. ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 168\u001b[0m\n\u001b[0;32m    164\u001b[0m     trainer\u001b[39m.\u001b[39mfit()\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 168\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[9], line 164\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39m# 트레이너 초기화 및 실행\u001b[39;00m\n\u001b[0;32m    151\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m    152\u001b[0m     TrainerArgs(\n\u001b[0;32m    153\u001b[0m         restore_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# xtts_checkpoint 키를 통해 xtts 체크포인트가 복원되므로 Trainer의 restore_path 파라미터를 사용해 복원할 필요가 없음\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     eval_samples\u001b[39m=\u001b[39meval_samples,\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         os\u001b[39m.\u001b[39m_exit(\u001b[39m1\u001b[39m)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m     remove_experiment_folder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_path)\n\u001b[0;32m   1861\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[0;32m   1862\u001b[0m     sys\u001b[39m.\u001b[39mexit(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\trainer\\generic_utils.py:77\u001b[0m, in \u001b[0;36mremove_experiment_folder\u001b[1;34m(experiment_path)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checkpoint_files:\n\u001b[0;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m fs\u001b[39m.\u001b[39mexists(experiment_path):\n\u001b[1;32m---> 77\u001b[0m         fs\u001b[39m.\u001b[39;49mrm(experiment_path, recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     78\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m ! Run is removed from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, experiment_path)\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\GitHub\\AI\\LangChain\\ChatBot-env\\lib\\site-packages\\fsspec\\implementations\\local.py:179\u001b[0m, in \u001b[0;36mLocalFileSystem.rm\u001b[1;34m(self, path, recursive, maxdepth)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m osp\u001b[39m.\u001b[39mabspath(p) \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetcwd():\n\u001b[0;32m    178\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot delete current working directory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 179\u001b[0m     shutil\u001b[39m.\u001b[39;49mrmtree(p)\n\u001b[0;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     os\u001b[39m.\u001b[39mremove(p)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:750\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    748\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m             os\u001b[39m.\u001b[39munlink(fullname)\n\u001b[0;32m    619\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m             onerror(os\u001b[39m.\u001b[39;49munlink, fullname, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[0;32m    621\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m     os\u001b[39m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:618\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m         os\u001b[39m.\u001b[39;49munlink(fullname)\n\u001b[0;32m    619\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    620\u001b[0m         onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: 'd:/GitHub/AI/LangChain/xTTS/run/training/GPT_XTTS_v2.0_BBANGHYONG_FT-June-24-2024_11+36PM-ad82aa3f\\\\trainer_0_log.txt'"
     ]
    }
   ],
   "source": [
    "# 훈련 문장 생성\n",
    "SPEAKER_REFERENCE = [\n",
    "    \"./wavs/audio2.wav\"  # 훈련 테스트 문장에서 사용할 화자 참조\n",
    "]\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "def main():\n",
    "    # args와 config 초기화\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=132300,  # 6초\n",
    "        min_conditioning_length=66150,  # 3초\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=255995,  # 약 11.6초\n",
    "        max_text_length=200,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,  # 파인튜닝할 모델의 체크포인트 경로\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "    # 오디오 설정 정의\n",
    "    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n",
    "    # 훈련 파라미터 설정\n",
    "    config = GPTTrainerConfig(\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"\"\"\n",
    "            GPT XTTS 훈련\n",
    "            \"\"\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=48,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=8,\n",
    "        eval_split_max_size=256,\n",
    "        print_step=50,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=10000,\n",
    "        save_n_checkpoints=1,\n",
    "        save_checkpoints=True,\n",
    "        # 목표 손실 설정 \"loss\"\n",
    "        print_eval=False,\n",
    "        # tortoise와 유사한 Optimizer 값, non-weight 파라미터에 WD를 적용하지 않도록 수정된 PyTorch 구현\n",
    "        optimizer=\"AdamW\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=5e-06,  # 학습률\n",
    "        lr_scheduler=\"StepLR\",\n",
    "        # 새로운 단계 체계에 맞게 조정됨\n",
    "        lr_scheduler_params={\"step_size\": 50, \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "        test_sentences=[\n",
    "            {\n",
    "                \"text\": \"나에게는 그들보다 이 점등인이 더 나은 사람이야. 적어도 점등인은 그들과는 달리, 남을 위해 일하기 때문이야. 너는 나에게 이 세상에 단 하나뿐인 존재가 되는 거고, 나도 너에게 세상에 하나뿐인 존재가 되는 거야.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"넌 네가 길들인 것에 영원히 책임이 있어. 누군가에게 길들여진다는 것은 눈물을 흘릴 일이 생긴다는 뜻일지도 몰라. 네 장미꽃이 너의 장미꽃이 되려면 너는 그녀를 길들여야 해. 그렇지 않으면 그녀에게는 오직 하나의 장미꽃일 뿐일 거야.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"정말로 내가 네게 필요한 것은 네가 가지고 있는 무기가 아니야. 내가 네게서 정말로 필요한 건 네가 수없이 열어준 문이야.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"너는 정말로 내게 신경 쓰지 않아. 또한 나에게 필요한 건 없고 또한 나의 많은 것을 나누기로 결정했어. 그냥 어떤 경우에나 내 행복을 위해 너를 원하지 않아. 나는 그런 사랑을 받고 싶어하지 않았어.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"난 당신이 아주 잘못 생각하고 있다고 생각해. 그러나 너는 나에게 더 많은 도움이 되고, 그렇게 도움이 필요할 때 마다 그들에게 전달 할 수있을 것입니다. 그렇지 않으면 그녀의 사랑과 나의 정확한 것을 말한다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"오늘날의 전세계 사람들이 모두 다른 곳에 살고 있어요. 그들은 모두 각자의 길을 걷고 있고, 모두 자신이 갈 곳을 알고 있어요. 그리고 그들은 그 길을 지나 살고 있어요.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"사실 그는 전체 원인을 보자고 그는 말하고, 그는 내게 얘기 했다. 그는 내게 말하고, 나는 내가 그렇게했다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"하루하루 시간이 되면 우리는 더 나은 미래를 찾기 위해 더 큰 노력을 계속할 수 있습니다. 또한 우리는 더 나은 시간을 보낼 것이고, 더 나은 미래를 만들 수 있습니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"이제는 모든 시간이 지나도 우리는 더 많은 노력을 기울여야 할 것이다. 또한 우리는 더 많은 노력을 기울여야 할 것이다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"현대 인류는 여행하는 중이며, 이곳을 탐험하는 중이며, 이 곳에서 살고 있습니다. 우리는 모두 각자의 방법으로 각자의 방법을 찾고, 그 방법을 갈 것입니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"당신의 여행이 끝날 때까지 당신의 여행은 여행이 될 것입니다. 여행이 끝나지 않을 때, 우리는 여행을 계속할 것입니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"당신의 집을 잃어버린다면, 당신은 당신의 집을 잃을 것입니다. 당신의 집을 잃어버린다면, 당신은 당신의 집을 잃을 것입니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"다른 나라에 살고 있습니다. 그리고 우리는 다른 사람들에게 기여할 것입니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"당신은 세계를 탐험하는 중입니다. 그리고 우리는 모두가 그것을 탐험하고 있습니다. 그리고 그것을 탐험하고 있습니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"당신은 당신의 모든 것을 잃어 버렸습니다. 그리고 우리는 그것을 찾을 수 있습니다.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 설정에서 모델 초기화\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "    # 훈련 샘플 로드\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "    )\n",
    "\n",
    "    # 트레이너 초기화 및 실행\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,  # xtts_checkpoint 키를 통해 xtts 체크포인트가 복원되므로 Trainer의 restore_path 파라미터를 사용해 복원할 필요가 없음\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
