{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# 150. Tensorflow Hub 를 사용한 영화 리뷰 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txOb1UvRgUWr"
   },
   "source": [
    "### 텐서플로 허브(TensorFlow Hub)와 케라스(Keras)를 사용한 기초적인 전이 학습(transfer learning)\n",
    "\n",
    "-  영화 리뷰(review) 텍스트를 *긍정*(positive) 또는 *부정*(negative)으로 분류. 이 예제는 *이진*(binary)-또는 클래스(class)가 두 개인- 분류 문제. \n",
    "\n",
    "\n",
    "-  텐서플로 허브(TensorFlow Hub)와 케라스(Keras)를 사용한 전이 학습(transfer learning) \n",
    "\n",
    "\n",
    "- [인터넷 영화 데이터베이스](https://www.imdb.com/)(Internet Movie Database)에서 수집한 50,000개의 영화 리뷰 텍스트를 담은 [IMDB 데이터셋](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)을 사용. 25,000개 리뷰는 훈련용으로, 25,000개는 테스트용. 긍정적인 리뷰와 부정적인 리뷰의 개수는 동일(균형 잡힌 dataset).  \n",
    "\n",
    "- 레이블(label)은 정수 0 or 1. 0 은 부정적인 리뷰, 1은 긍정적 리뷰.\n",
    "\n",
    "\n",
    "- [tf.keras](https://www.tensorflow.org/guide/keras)와 전이 학습 라이브러리이자 플랫폼인 [텐서플로 허브](https://www.tensorflow.org/hub)를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버전:  2.5.1\n",
      "허브 버전:  0.9.0\n",
      "GPU NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"버전: \", tf.__version__)\n",
    "print(\"허브 버전: \", hub.__version__)\n",
    "print(\"GPU\", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## IMDB 데이터셋 다운로드\n",
    "\n",
    "- IMDB 데이터셋은 [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) 또는 [텐서플로 데이터셋](https://www.tensorflow.org/datasets)(TensorFlow datasets)에 포함되어 있다. \n",
    "\n",
    "- 훈련 세트를 6대 4로 나눔 \n",
    "- train 15,000, validation 10,000, test 25,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "train_data, test_data = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## 데이터 탐색\n",
    "\n",
    "- 처음 3개의 샘플을 출력해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      " b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n",
      " b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'], shape=(3,), dtype=string)\n",
      "\n",
      "tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(3)))\n",
    "print(train_examples_batch)\n",
    "print()\n",
    "print(train_labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## 모델 구성\n",
    "\n",
    "\n",
    "- 첫 번째 층으로 사전 훈련(pre-trained)된 텍스트 임베딩을 사용  |\n",
    "\n",
    "\n",
    "- pre-trained embedding 의 장점\n",
    "\n",
    "    * 텍스트 전처리에 대해 신경 쓸 필요가 없음\n",
    "    * 전이 학습의 장점을 이용합니다.\n",
    "    * 임베딩은 고정 크기이기 때문에 처리 과정이 단순해집니다.\n",
    "    \n",
    "\n",
    "- [텐서플로 허브](https://www.tensorflow.org/hub)에 있는 **사전 훈련된 텍스트 임베딩 모델**인 [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)을 사용\n",
    "\n",
    "    - English Google News 130GB corpus 에서 train 된 text embedding \n",
    "\n",
    "테스트해 볼 수 있는 사전 훈련된 모델이 세 개 더 있습니다:\n",
    "\n",
    "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) --> \n",
    "[google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)와 동일하지만 어휘 사전(vocabulary)의 2.5%가 OOV 버킷(bucket)으로 변환되었음. 이는 해당 문제의 어휘 사전과 모델의 어휘 사전이 완전히 겹치지 않을 때 도움이 된다.\n",
    "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) --> 더 큰 모델. 차원 크기는 50이고 어휘 사전의 크기는 1백만 개 이하.\n",
    "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) --> 훨씬 더 큰 모델. 차원 크기는 128이고 어휘 사전의 크기는 1백만 개 이하."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In2nDpTLkgKa"
   },
   "source": [
    "- 문장을 임베딩시키기 위해 텐서플로 허브 모델을 사용하는 케라스 층을 작성하고 몇 개의 샘플을 입력하여 테스트.  \n",
    "- 입력 텍스트의 길이에 상관없이 임베딩의 출력 크기는 `(num_examples, embedding_dimension)`가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NUbzVeYkgcO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
       "array([[ 1.765786  , -3.882232  ,  3.9134233 , -1.5557289 , -3.3362343 ,\n",
       "        -1.7357955 , -1.9954445 ,  1.2989551 ,  5.081598  , -1.1041286 ,\n",
       "        -2.0503852 , -0.72675157, -0.65675956,  0.24436149, -3.7208383 ,\n",
       "         2.0954835 ,  2.2969332 , -2.0689783 , -2.9489717 , -1.1315987 ],\n",
       "       [ 1.8804485 , -2.5852382 ,  3.4066997 ,  1.0982676 , -4.056685  ,\n",
       "        -4.891284  , -2.785554  ,  1.3874227 ,  3.8476458 , -0.9256538 ,\n",
       "        -1.896706  ,  1.2113281 ,  0.11474707,  0.76209456, -4.8791065 ,\n",
       "         2.906149  ,  4.7087674 , -2.3652055 , -3.5015898 , -1.6390051 ],\n",
       "       [ 0.71152234, -0.6353217 ,  1.7385626 , -1.1168286 , -0.5451594 ,\n",
       "        -1.1808156 ,  0.09504455,  1.4653089 ,  0.66059524,  0.79308075,\n",
       "        -2.2268345 ,  0.07446612, -1.4075904 , -0.70645386, -1.907037  ,\n",
       "         1.4419787 ,  1.9551861 , -0.42660055, -2.8022065 ,  0.43727064]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "hub_layer(train_examples_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfSbV6igl1EH"
   },
   "source": [
    "### Model 생성\n",
    "\n",
    "\n",
    "1. 첫 번째 층은 텐서플로 허브 층. 이 층은 사전 훈련된 모델을 사용하여 하나의 문장을 임베딩 벡터로 매핑.   \n",
    "    - 여기서 사용하는 사전 훈련된 텍스트 임베딩 모델([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1))은 하나의 문장을 토큰(token)으로 나누고 각 토큰의 임베딩을 연결하여 반환\n",
    "    - 최종 차원은 `(num_examples, embedding_dimension)`\n",
    "    \n",
    "    \n",
    "2. 이 고정 크기의 출력 벡터는 16개의 은닉 유닛(hidden unit)을 가진 완전 연결 층(`Dense`)으로 주입된다.  \n",
    "\n",
    "\n",
    "3. 마지막 층은 하나의 출력 노드를 가진 완전 연결 층. `sigmoid` 활성화 함수를 사용하므로 확률 또는 신뢰도 수준을 표현하는 0~1 사이의 실수 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1344      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 401,429\n",
      "Trainable params: 401,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수와 옵티마이저\n",
    "\n",
    "- 이진 분류 문제이고 모델이 확률을 출력하므로 `binary_crossentropy` 손실 함수를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "- 512개의 샘플로 이루어진 미니배치(mini-batch)에서 20번의 에포크(epoch) 동안 훈련  \n",
    "\n",
    "- 훈련하는 동안 10,000개의 검증 세트에서 모델의 손실과 정확도를 모니터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohyoungjea/anaconda3/envs/tf20/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:5017: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 4s 61ms/step - loss: 0.7366 - accuracy: 0.5553 - val_loss: 0.6288 - val_accuracy: 0.6469\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 3s 60ms/step - loss: 0.5853 - accuracy: 0.6911 - val_loss: 0.5595 - val_accuracy: 0.7142\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 3s 60ms/step - loss: 0.5161 - accuracy: 0.7537 - val_loss: 0.4993 - val_accuracy: 0.7676\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 3s 67ms/step - loss: 0.4526 - accuracy: 0.7984 - val_loss: 0.4453 - val_accuracy: 0.7986\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 3s 67ms/step - loss: 0.3960 - accuracy: 0.8301 - val_loss: 0.4029 - val_accuracy: 0.8211\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 4s 71ms/step - loss: 0.3515 - accuracy: 0.8511 - val_loss: 0.3730 - val_accuracy: 0.8345\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.3138 - accuracy: 0.8712 - val_loss: 0.3515 - val_accuracy: 0.8466\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.2862 - accuracy: 0.8843 - val_loss: 0.3370 - val_accuracy: 0.8528\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 4s 77ms/step - loss: 0.2607 - accuracy: 0.8962 - val_loss: 0.3233 - val_accuracy: 0.8600\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.2392 - accuracy: 0.9066 - val_loss: 0.3161 - val_accuracy: 0.8636\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 4s 78ms/step - loss: 0.2207 - accuracy: 0.9148 - val_loss: 0.3113 - val_accuracy: 0.8672\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.2052 - accuracy: 0.9224 - val_loss: 0.3097 - val_accuracy: 0.8683\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 4s 76ms/step - loss: 0.1893 - accuracy: 0.9297 - val_loss: 0.3099 - val_accuracy: 0.8702\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 4s 76ms/step - loss: 0.1758 - accuracy: 0.9353 - val_loss: 0.3107 - val_accuracy: 0.8706\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 4s 76ms/step - loss: 0.1639 - accuracy: 0.9420 - val_loss: 0.3138 - val_accuracy: 0.8703\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 4s 80ms/step - loss: 0.1539 - accuracy: 0.9460 - val_loss: 0.3181 - val_accuracy: 0.8690\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 4s 78ms/step - loss: 0.1419 - accuracy: 0.9514 - val_loss: 0.3304 - val_accuracy: 0.8660\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 4s 76ms/step - loss: 0.1330 - accuracy: 0.9550 - val_loss: 0.3319 - val_accuracy: 0.8678\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.1235 - accuracy: 0.9601 - val_loss: 0.3383 - val_accuracy: 0.8664\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 4s 73ms/step - loss: 0.1155 - accuracy: 0.9626 - val_loss: 0.3498 - val_accuracy: 0.8648\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=test_data.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## 모델 평가\n",
    "\n",
    "- 손실과 정확도 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 - 2s - loss: 0.3498 - accuracy: 0.8648\n",
      "loss: 0.350\n",
      "accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "이 예제는 매우 단순한 방식으로 87% 정도의 정확도를 달성했습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLdZN4nP4ano"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03158325]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"It's terrible. I would not recommend the movie.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7d50aO46Ka_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99139166]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['The movie was fantastic. Have fun.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C9ty5CN7hLt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5590752]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['The animation was out of the world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQ_3UvPU7mno"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "150_text_classification_with_hub.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
