{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 010. Vectorization of Statement (문장의 vector 화)\n",
    "\n",
    "- BOW (Bag of Words) \n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency)  \n",
    "- Word Embedding - Keras word API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = ['I love my dog.', \n",
    "             'I love my cat.', \n",
    "             'I love my dog and love my cat',\n",
    "             'You love my dog!',\n",
    "             'Do you think my dog is amazing?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Word (BOW)\n",
    "\n",
    "- CountVectorizer\n",
    "    - min_df : vocabulary 에 포함할 최소 발생 빈도\n",
    "    - ngram_range : (1, 1) - unigram only, (1, 2) - unigram + bigram\n",
    "    - max_features : top max_features 만으로 vocabulary 구성\n",
    "    - token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b : unocode 영수자 2 글자 이상만 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vs token Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = count_vectorizer.fit_transform(sentences)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 수: 5\n",
      "단어수: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"document 수: {features.shape[0]}\")\n",
    "print(f\"단어수: {features.shape[1]-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 2, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_sentences = features.toarray()\n",
    "vectorized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features 의 단어 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing', 'and', 'cat', 'do', 'dog', 'is', 'love', 'my', 'think', 'you']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>do</th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>think</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          amazing  and  cat  do  dog  is  love  my  think  you\n",
       "sentence                                                      \n",
       "0               0    0    0   0    1   0     1   1      0    0\n",
       "1               0    0    1   0    0   0     1   1      0    0\n",
       "2               0    1    1   0    1   0     2   2      0    0\n",
       "3               0    0    0   0    1   0     1   1      0    1\n",
       "4               1    0    0   1    1   1     0   1      1    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(vectorized_sentences, columns=feature_names)\n",
    "df.index.name = 'sentence'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF\n",
    "\n",
    "- TF-IDF(Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vs tf-idf Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.60685614,\n",
       "        0.        , 0.60685614, 0.51327503, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.73792244, 0.        , 0.        ,\n",
       "        0.        , 0.51528988, 0.43582888, 0.        , 0.        ],\n",
       "       [0.        , 0.49110884, 0.39622352, 0.        , 0.27668216,\n",
       "        0.        , 0.55336431, 0.46803199, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.45805379,\n",
       "        0.        , 0.45805379, 0.38741896, 0.        , 0.65595732],\n",
       "       [0.43872423, 0.        , 0.        , 0.43872423, 0.24716958,\n",
       "        0.43872423, 0.        , 0.20905445, 0.43872423, 0.35395995]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_sentences = tfidf_sentences.toarray()\n",
    "tfidf_vect_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing', 'and', 'cat', 'do', 'dog', 'is', 'love', 'my', 'think', 'you']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>do</th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>think</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.513275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.737922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515290</td>\n",
       "      <td>0.435829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491109</td>\n",
       "      <td>0.396224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553364</td>\n",
       "      <td>0.468032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.387419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.247170</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209054</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.353960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing       and       cat        do       dog        is      love  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.606856  0.000000  0.606856   \n",
       "1  0.000000  0.000000  0.737922  0.000000  0.000000  0.000000  0.515290   \n",
       "2  0.000000  0.491109  0.396224  0.000000  0.276682  0.000000  0.553364   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.458054  0.000000  0.458054   \n",
       "4  0.438724  0.000000  0.000000  0.438724  0.247170  0.438724  0.000000   \n",
       "\n",
       "         my     think       you  \n",
       "0  0.513275  0.000000  0.000000  \n",
       "1  0.435829  0.000000  0.000000  \n",
       "2  0.468032  0.000000  0.000000  \n",
       "3  0.387419  0.000000  0.655957  \n",
       "4  0.209054  0.438724  0.353960  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tfidf_vect_sentences, columns=tfidf_feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. keras word encoding\n",
    "\n",
    "- keras  API 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')    # 문장으로 부터 상위 100 개 단어로 vocabulary 작성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Index Vocabulary 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'cat': 6,\n",
       " 'you': 7,\n",
       " 'and': 8,\n",
       " 'do': 9,\n",
       " 'think': 10,\n",
       " 'is': 11,\n",
       " 'amazing': 12}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text 의 sentence 변환 및 paddding\n",
    "\n",
    "- texts_to_sequences: text list 내의 각 text 를 수열 (sequence of integers) 로 convert\n",
    "\n",
    "\n",
    "    - 입력 : text (strings) list\n",
    "    - 반환 : sequence list\n",
    "    \n",
    "- pad_sequences: 동일한 길이로 sequence 를 zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 6], [5, 3, 2, 4, 8, 3, 2, 6], [7, 3, 2, 4], [9, 7, 10, 2, 4, 11, 12]]\n",
      "\n",
      "[[ 5  3  2  4  0  0  0  0]\n",
      " [ 5  3  2  6  0  0  0  0]\n",
      " [ 5  3  2  4  8  3  2  6]\n",
      " [ 7  3  2  4  0  0  0  0]\n",
      " [ 9  7 10  2  4 11 12  0]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "print()\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<OOV>',\n",
       " 2: 'my',\n",
       " 3: 'love',\n",
       " 4: 'dog',\n",
       " 5: 'i',\n",
       " 6: 'cat',\n",
       " 7: 'you',\n",
       " 8: 'and',\n",
       " 9: 'do',\n",
       " 10: 'think',\n",
       " 11: 'is',\n",
       " 12: 'amazing'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequenced sentence 를 word sentence 로 환원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog\n",
      "i love my cat\n",
      "i love my dog and love my cat\n",
      "you love my dog\n",
      "do you think my dog is amazing\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences:\n",
    "    sent = []\n",
    "    for idx in sequence:\n",
    "        sent.append(tokenizer.index_word[idx])\n",
    "    print(' '.join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
