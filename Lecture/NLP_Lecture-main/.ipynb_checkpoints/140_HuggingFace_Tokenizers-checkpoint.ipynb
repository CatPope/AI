{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "constant-affairs",
   "metadata": {},
   "source": [
    "# 140.  각 tokenization algorithm 파악\n",
    "\n",
    "## Huggin Face Tokenizers\n",
    "\n",
    "- transformers에서는 tokenizer 학습 기능을 제공 않고, 모든 tokenizer는 pre-trained tokenizer라고 가정하므로 별도로 Hugging Face의 tokenizers를 이용하여 tokenizer를 학습해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "average-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-analyst",
   "metadata": {},
   "source": [
    "# Hugging Face 가 제공하는 tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "communist-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(tokenizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "framed-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(tokenizers.BertWordPieceTokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-symphony",
   "metadata": {},
   "source": [
    "## Byte-level BPE(Byte Pair Encoding) Tokenizer\n",
    "\n",
    "- OpenAI GPT2에 사용된 tokenizer  \n",
    "\n",
    "- 사전 토큰화 후 고유한 단어 세트가 생성되고 훈련 데이터에서 발생하는 각 단어의 빈도가 결정됩니다. 다음으로 BPE는 고유한 단어 집합에서 발생하는 모든 기호로 구성된 기본 어휘를 만들고 기본 어휘의 두 기호에서 새로운 기호를 형성하는 병합 규칙을 학습합니다. 어휘가 원하는 어휘 크기에 도달할 때까지 그렇게 합니다. 원하는 어휘 크기는 토크나이저를 훈련하기 전에 정의할 하이퍼파라미터입니다.\n",
    "\n",
    "- Byte-level BPE 는 글자가 아닌 byte 기준으로 BPE 를 적용하기 때문에 1 byte 로 표현되는 글자 (알파벳, 숫자, 기호)만 형태가 보존됩니다.\n",
    "\n",
    "- 모든 유니코드 문자는 기본 문자로 간주됩니다. 더 나은 기본 어휘를 갖기 위해 GPT-2는 바이트를 기본 어휘로 사용합니다. 이는 기본 어휘를 256 크기로 강제하면서 모든 기본 문자가 어휘에 포함되도록 하는 영리한 트릭입니다. \n",
    "\n",
    "- 구두점을 처리하기 위한 몇 가지 추가 규칙으로 GPT2의 토크나이저는 `<unk>` 기호 없이 모든 텍스트를 토큰화할 수 있습니다.\n",
    "\n",
    "\n",
    "- GPT-2의 어휘 크기는 50,257이며, 이는 256바이트 기본 토큰, special end-of-text 토큰 및 50,000개의 병합으로 학습된 기호에 해당합니다.\n",
    "\n",
    "\n",
    "- 띄어쓰기로 시작하는 단어 앞에 Ġ 를 prefix 로 부착합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considerable-default",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDE ABC AC ABD\\nDE AB ABC AF'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_corpus = './data/very_small_alphabet_corpus.txt'\n",
    "f = open('./data/very_small_alphabet_corpus.txt', 'r')\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "commercial-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ĠA', 'ĠAB', 'ĠABC', 'DE', 'ĠDE', 'ĠAC', 'ĠAF', 'ĠABD', 'ĠABCDE']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "bytebpe_tokenizer = ByteLevelBPETokenizer(add_prefix_space=True)\n",
    "\n",
    "bytebpe_tokenizer.train(files = [small_corpus],\n",
    "    vocab_size = 1000, min_frequency = 1)\n",
    "\n",
    "vocab = bytebpe_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reported-ethernet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressed-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠABCDE', 'ĠABC']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytebpe_tokenizer.encode('ABCDE ABC').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-hearing",
   "metadata": {},
   "source": [
    "## Bert WordPiece Tokenizer  \n",
    "\n",
    "- BERT 에 사용된 tokenization 알고리즘  \n",
    "\n",
    "- 일본어, 한국어등에도 적용 가능  \n",
    "\n",
    "- WordPiece는 먼저 학습 데이터에 있는 모든 문자를 포함하도록 어휘를 초기화하고, 주어진 수의 병합 규칙을 점진적으로 학습합니다. BPE와 달리 WordPiece는 가장 빈번한 기호 쌍을 선택하지 않고 일단 어휘에 추가된 훈련 데이터의 가능성을 최대화하는 기호 쌍을 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-noise",
   "metadata": {},
   "source": [
    "Huggingface의 Tokenizer trainer는 학습시 다음과 같은 인자를 갖는다\n",
    "\n",
    "min_frequency : merge를 수행할 최소 빈도수, 5로 설정 시 5회 이상 등장한 pair만 수행한다  \n",
    "vocab_size: 만들고자 하는 vocab의 size, 보통 '32000' 정도가 좋다고 알려져 있다  \n",
    "show_progress : 학습 진행과정 show  \n",
    "special_tokens : Tokenizer에 추가하고 싶은 special token 지정  \n",
    "limit_alphabet : merge 수행 전 initial tokens이 유지되는 숫자 제한  \n",
    "initial_alphabet : 꼭 포함됐으면 하는 initial alphabet, 이곳에 설정한 token은 학습되지 않고 그대로 포함되도록 설정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alert-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 5, 'e': 9, '##b': 11, 'd': 8, '##d': 12, '[MASK]': 4, '[PAD]': 0, '##c': 13, '[SEP]': 3, 'c': 7, '##f': 15, '[UNK]': 1, '[CLS]': 2, 'b': 6, 'f': 10, '##e': 14}\n",
      "\n",
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', '##b', '##d', '##c', '##e', '##f']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "# train tokenizer\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 10,\n",
    "    min_frequency = 1,\n",
    "    limit_alphabet = 1000,\n",
    "    initial_alphabet = [],\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress = True,\n",
    "    wordpieces_prefix = \"##\",\n",
    ")\n",
    "\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "\n",
    "print(vocab)\n",
    "print()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-outside",
   "metadata": {},
   "source": [
    "- encode() method를 이용하여 sentence를 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##b', '##c', '##d', '##e', 'a', '##b', '##c']\n",
      "[5, 11, 13, 12, 14, 5, 11, 13]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE ABC')\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-robinson",
   "metadata": {},
   "source": [
    "### 대문자 그대로 tokenize\n",
    "\n",
    "- lowercase = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "executive-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '##B', '##C', '##D', '##E']\n",
      "[5, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(lowercase = False)\n",
    "\n",
    "bert_wordpiece_tokenizer.train(files=[small_corpus], vocab_size=10)\n",
    "\n",
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-statistics",
   "metadata": {},
   "source": [
    "### vocab size 를 키우면 더 많은 subword 가 vocab으로 학습된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "radical-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##c', '##f', '##b', '##d', '##e', 'ab', 'abc', 'ac']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    initial_alphabet = ['g'],\n",
    ")\n",
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-belarus",
   "metadata": {},
   "source": [
    "### sentence 를 list 로 입력 받아 batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "native-supplement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', '##d', '##e']\n",
      "['abc', '##d']\n"
     ]
    }
   ],
   "source": [
    "encodings = bert_wordpiece_tokenizer.encode_batch(['ABCDE', 'abcd'])\n",
    "\n",
    "print(encodings[0].tokens)\n",
    "print(encodings[1].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alpine-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./huggingface_output/very_small_bertwordpiece-vocab.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './huggingface_output',\n",
    "    prefix='very_small_bertwordpiece'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "annoying-taste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##d', '##e', 'abc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode('ABCDE ABC').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-rolling",
   "metadata": {},
   "source": [
    "### 저장했던 tokenizer 를 load 하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lightweight-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', 'abc', '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab = './huggingface_output/very_small_bertwordpiece-vocab.txt'\n",
    ")\n",
    "\n",
    "bert_wordpiece_tokenizer.encode('ABCDE ABC', add_special_tokens=True).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "canadian-merit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##d', '##e', 'abc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode('ABCDE ABC', add_special_tokens=False).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-blood",
   "metadata": {},
   "source": [
    "### BertWordPieceTokenizer 는 두개의 문장을 pair 하는 기능 제공\n",
    "\n",
    "- BERT 의 next sentence prediction 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chief-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##d', '##e', '[SEP]', 'abc', '##d', '[SEP]']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode(\n",
    "    sequence='abcde',\n",
    "    pair = 'abcd'\n",
    ").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-dinner",
   "metadata": {},
   "source": [
    "## SentencePiece BPE(Byte Pair Encoding) Tokenizer\n",
    "\n",
    "- 지금까지 설명한 모든 토큰화 알고리즘에는 동일한 문제가 있습니다. 입력 텍스트가 공백을 사용하여 단어를 구분한다고 가정합니다. 그러나 모든 언어에서 공백을 사용하여 단어를 구분하는 것은 아닙니다. 중국어, 한국어, 일본어 및 태국어는 dictionary 형태의 형태소 분리기를 사용하여 문제를 해결합니다. \n",
    "\n",
    "- 이 문제를 보다 일반적으로 해결하기 위해 SentencePiece 는 입력을 원시 입력 스트림으로 처리하므로 사용할 문자 집합에 공백을 포함합니다. 그런 다음 BPE 알고리즘을 사용하여 적절한 어휘를 구성합니다.\n",
    "\n",
    "\n",
    "- add_prefix_space=True 이면 문장의 맨 앞 단어에도 공백을 부여, False 이면 공백없이 시작하는 단어에는 ▁ 를 붙이지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ideal-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', '▁ABC', 'DE', 'D\\n', '▁DE', '▁AC', '▁AF', '▁ABD\\n', '▁ABCDE']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space = True)\n",
    "# tokenizer train\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>'],)\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "substantial-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', 'DE', '▁ABC', 'AB', 'CDE', 'D\\n', '▁AC', '▁AF', '▁ABD\\n', 'ABCDE']\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space = False)\n",
    "\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [small_corpus],\n",
    "    vocab_size = 20,\n",
    "    min_frequency = 1,\n",
    "    special_tokens = ['<unk>'],)\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-community",
   "metadata": {},
   "source": [
    "## Tokenizer alogorithm 별 결과 비교\n",
    "\n",
    "- 동일한 코로나19 관련 뉴스를 학습하여 tokenizer 별 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "returning-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "corpus_path = './data/2020-07-29_covid_news_sents.txt'\n",
    "vocab_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fiscal-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./huggingface_output/ByteLevelBPETokenizer-covid-vocab.json',\n",
       " './huggingface_output/ByteLevelBPETokenizer-covid-merges.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_level_bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "byte_level_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "byte_level_bpe_tokenizer.save_model(directory='./huggingface_output', prefix='ByteLevelBPETokenizer-covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "resistant-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./huggingface_output/SentencePieceBPETokenizer-covid-vocab.json',\n",
       " './huggingface_output/SentencePieceBPETokenizer-covid-merges.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_bpe_tokenizer = SentencePieceBPETokenizer()\n",
    "sentencepiece_bpe_tokenizer.train(files=[corpus_path], vocab_size=vocab_size)\n",
    "sentencepiece_bpe_tokenizer.save_model(directory='./huggingface_output', prefix='SentencePieceBPETokenizer-covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "decent-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./huggingface_output/BertWordPieceTokenizer-covid-vocab.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files=[corpus_path], vocab_size=vocab_size)\n",
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory='./huggingface_output', prefix='BertWordPieceTokenizer-covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adolescent-poetry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BertWordPieceTokenizer\n",
      "tokens = ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "\n",
      "SentencePieceBPETokenizer\n",
      "tokens = ['▁신종', '▁코로나바이러스', '▁감염증(코로나19)', '▁사태', '가', '▁심', '각', '합', '니다']\n",
      "\n",
      "ByteLevelBPETokenizer\n",
      "tokens = ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "tokenizers = [bert_wordpiece_tokenizer,\n",
    "              sentencepiece_bpe_tokenizer,\n",
    "              byte_level_bpe_tokenizer]\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    encode_single = tokenizer.encode(sent_ko)\n",
    "    print(f'\\n{tokenizer.__class__.__name__}')\n",
    "    print(f'tokens = {encode_single.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-tender",
   "metadata": {},
   "source": [
    "## 학습한 토크나이저를 transformers 에서 이용\n",
    "\n",
    "- transformers 라이브러리는 BertTokenizer 와 GPT2Tokenizer 를 별도로 제공  \n",
    "\n",
    "- BertTokenizer 는 BertWordPieceTokenizer 와 동일  \n",
    "\n",
    "- GPT2Tokenizer 는 ByteLevelBPETokenizer 와 동일\n",
    "\n",
    "- 이를 확인하기 위해, 위에서 train 한 bert_wordpiece_tokenizer 및 byte_level_bpe_tokenizer 에 코로나 19 뉴스로 학습하여 저장한 vocab file을 load한 tokenizer의 tokenization 결과가 동일함을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "exclusive-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "# train 된 tokenizer model load\n",
    "transformers_bert_tokenizer = BertTokenizer(\n",
    "    vocab_file = './huggingface_output/BertWordPieceTokenizer-covid-vocab.txt'\n",
    ")\n",
    "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "egyptian-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n",
      "transformers: ['ìĭłì¢ħ', 'Ġì½Ķë¡ľëĤĺë°ĶìĿ´ëŁ¬ìĬ¤', 'Ġê°ĲìĹ¼ì¦Ŀ', '(', 'ì½Ķë¡ľëĤĺ', '19', ')', 'ĠìĤ¬íĥľ', 'ê°Ģ', 'Ġìĭ¬', 'ê°ģ', 'íķ©ëĭĪëĭ¤']\n"
     ]
    }
   ],
   "source": [
    "transformers_gpt2_tokenizer = GPT2Tokenizer(\n",
    "    vocab_file = './huggingface_output/ByteLevelBPETokenizer-covid-vocab.json',\n",
    "    merges_file = './huggingface_output/ByteLevelBPETokenizer-covid-merges.txt'\n",
    ")\n",
    "print(f'tokenizers  : {byte_level_bpe_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_gpt2_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-winner",
   "metadata": {},
   "source": [
    "### 참고) 한글 처리\n",
    "\n",
    "- 화면에서 한글로 보이지만 실제로는 자/모 분해가 된 글자입니다. length 를 print 해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "complicated-interaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(신종) = 6\n",
      "len(코로나바이러스) = 14\n",
      "len(감염증) = 9\n"
     ]
    }
   ],
   "source": [
    "for token in bert_wordpiece_tokenizer.encode(sent_ko).tokens[:3]:\n",
    "    print(f'len({token}) = {len(token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-catholic",
   "metadata": {},
   "source": [
    "### unicodedata.normalize  \n",
    "\n",
    "- ‘NFKD’ 는 한글의 자/모를 분해, ‘NFKC’ 는 자/모를 한글로 조합합니다. 보이는 것과 글자 길이가 같아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "postal-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가감\n",
      "5\n",
      "가감\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "print(normalize('NFKD', '가감'))  # 출력 시 글자를 재조합해서 보여줌\n",
    "print(len(normalize('NFKD', '가감')))\n",
    "\n",
    "print(normalize('NFKC', normalize('NFKD', '가감')))\n",
    "print(len(normalize('NFKC', normalize('NFKD', '가감'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-uruguay",
   "metadata": {},
   "source": [
    "매번 NFKC 로 후처리를 반복해야 한다면 간단한 함수를 만들어 두는 것이 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "broad-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n",
      "transformers: ['신종', '코로나바이러스', '감염증', '(', '코로나19', ')', '사태', '##가', '심', '##각', '##합니다']\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def compose(tokens):\n",
    "    return [normalize('NFKC', token) for token in tokens]\n",
    "\n",
    "print(f'tokenizers  : {compose(bert_wordpiece_tokenizer.encode(sent_ko).tokens)}')\n",
    "print(f'transformers: {compose(transformers_bert_tokenizer.tokenize(sent_ko))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
