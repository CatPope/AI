{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whjogPl1KL4-"
   },
   "source": [
    "# 038. Hugging Face Tokenizer Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 단어 사전 기반 한국어 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코로나', '가', '심하다']\n",
      "['코', '비드', '-', '19', '가', '심하다']\n",
      "['아버지', '가방', '에', '들어가신다']\n",
      "['아버지', '가', '방', '에', '들어가신다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "sentences = [\n",
    "    \"코로나가 심하다\",\n",
    "    \"코비드-19가 심하다\",\n",
    "    '아버지가방에들어가신다',\n",
    "    '아버지가 방에 들어가신다'\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(okt.morphs(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Okt 사전에 미등록된 단어의 경우 정확한 tokenizing 이 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('너무', 'Adverb'),\n",
       " ('너무', 'Adverb'),\n",
       " ('너', 'Modifier'),\n",
       " ('무', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('나카무라', 'Noun'),\n",
       " ('세이', 'Noun'),\n",
       " ('코', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('불러', 'Verb'),\n",
       " ('크게', 'Noun'),\n",
       " ('히트', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('노래', 'Noun'),\n",
       " ('입니다', 'Adjective')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos('너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 `너무너무너무`와 `나카무라세이코`는 하나의 단어이지만, okt 사전에 등록되어 있지 않아 여러 개의 복합단어로 나뉘어집니다. 이러한 문제를 해결하기 위하여 형태소 분석기와 품사 판별기들은 사용자 사전 추가 기능을 제공합니다. 사용자 사전을 추가하여 모델의 vocabulary 를 풍부하게 만드는 것은 사용자의 몫입니다.\n",
    "\n",
    "1. okt 공식 문서를 참고해서 사용사 사전을 추가.\n",
    "2. okt를 패키징하고, konlpy에서 사용할 수 있도록 konlpy/java 경로에 jar 파일을 복사.\n",
    "3. 기존에 참고하고 있던 okt.jar 대신 새로운 okt.jar를 사용하도록 설정.\n",
    "4. konlpy 소스 경로를 import 해서 형태소 분석."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Keras Tokenizer - rule-based\n",
    "- 공백 또는 구둣점으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')  # 빈도수 상위 100 개로 구성\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'I was born in Korea and graduaged University in USA.',\n",
    "    '너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'in': 6, 'cat': 7, 'you': 8, 'was': 9, 'born': 10, 'korea': 11, 'and': 12, 'graduaged': 13, 'university': 14, 'usa': 15, '너무너무너무는': 16, '나카무라세이코가': 17, '불러': 18, '크게': 19, '히트한': 20, '노래입니다': 21}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)           \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hugging Face tokenizers \n",
    "\n",
    "- 네 가지 토크나이저(Byte-level BPE, Character-level BPE, SentencePiece, Bert Tokenizer) 모두 BaseTokenizer 를 상속하며, 아래의 기능이 구현되어 있다.\n",
    "\n",
    "    • get_vocab()  \n",
    "    • add_tokens(), add_special_tokens()  \n",
    "    • decode() / decode_batch()  \n",
    "    • save() / save_model()   \n",
    "\n",
    "### 2-1. WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XI3JPwU37zI7"
   },
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"very_small_corpus.txt\", \"w\") as f:\n",
    "    f.write(\"ABCDE ABC AC ABD\\n\")\n",
    "    f.write(\"DE AB ABC AF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = [\"very_small_corpus.txt\"],\n",
    "    vocab_size = 10,           #만드려는 vocab의 size (보통 32000)\n",
    "    min_frequency = 1,         #merge를 수행할 최소 빈도수\n",
    "    limit_alphabet = 1000,     #merge 수행 전 initial token에 유지되는 숫자 제한 ([UNK]를 줄이기 위해 큰 숫자 지정)\n",
    "    initial_alphabet = ['g'],     #반드시 포함하고 싶은 alphabet\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress = True,\n",
    "    wordpieces_prefix = \"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[MASK]': 4, 'a': 5, 'b': 6, '##f': 13, 'c': 7, 'd': 8, 'g': 11, '##b': 14, '##c': 15, '[PAD]': 0, '[SEP]': 3, 'e': 9, '[UNK]': 1, 'f': 10, '##e': 12, '[CLS]': 2, '##d': 16}\n"
     ]
    }
   ],
   "source": [
    "vocab = bert_wordpiece_tokenizer.get_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', '##e', '##f', '##b', '##c', '##d']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize() 의 기능이 없고 대신 encode() 를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##b', '##c', '##d', '##e']\n",
      "[5, 14, 15, 16, 12]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode('ABCDE')\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode_batch 는 list of str 을 입력받아 list of Encoding 을 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', '##b', '##c', '##d', '##e'], ['a', '##b', '##c', '##d']]\n",
      "[[5, 14, 15, 16, 12], [5, 14, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode_batch(['ABCDE', 'abcd'])\n",
    "\n",
    "print([encoding[i].tokens for i in range(len(encoding))])\n",
    "print([encoding[i].ids for i in range(len(encoding))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_model 을 이용하여 {directory}/{name}-vocab.txt 파일로 vocab 을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./very_small_bert_wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.save_model(\n",
    "    directory = './',\n",
    "    prefix = 'very_small_bert_wordpiece'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-train 된 vocab 을 불러와 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', '##b', '##c', '##d', '[SEP]']\n",
      "['a', '##b', '##c', '##d']\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab = 'very_small_bert_wordpiece-vocab.txt'\n",
    ")\n",
    "\n",
    "print(bert_wordpiece_tokenizer.encode('ABCD').tokens)\n",
    "print(bert_wordpiece_tokenizer.encode('ABCD', add_special_tokens=False).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT 는 두 문장을 [SEP] 으로 구분하며 학습하기 때문에 pair 기능을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', '##b', '##c', '##d', '##e', '[SEP]', 'a', '##b', '##c', '##d', '[SEP]']\n",
      "[2, 5, 14, 15, 16, 12, 3, 5, 14, 15, 16, 3]\n"
     ]
    }
   ],
   "source": [
    "encoding = bert_wordpiece_tokenizer.encode(\n",
    "    sequence = 'abcde',\n",
    "    pair = 'abcd'\n",
    ")\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. SentencePieceBPE Tokenizer \n",
    "\n",
    "- SentencePiece 는 공백 뒤에 등장하는 단어 앞에 ▁ 를 붙여 실제 공백과 subwords 의 경계를 구분합니다.  \n",
    "\n",
    "\n",
    "- add_prefix_space=True 이면 문장의 맨 앞 단어에도 공백을 부여, False 이면 공백없이 시작하는 단어에는 ▁ 를 붙이지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space=True)\n",
    "\n",
    "sentencepiece_tokenizer.train(\n",
    "    files = [\"very_small_corpus.txt\"],\n",
    "    vocab_size = 20,           \n",
    "    min_frequency = 1,         \n",
    "    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', '▁', '▁A', '▁AB', '▁ABC', 'DE', 'D\\n', '▁DE', '▁AC']\n"
     ]
    }
   ],
   "source": [
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE 를 이용하는 토크나이저들은 `vocab.json` 과 `merges.txt` 두 개의 파일을 저장합니다.\n",
    "\n",
    "학습된 토크나이저를 이용할 때에도 두 개의 파일을 모두 입력해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./very_small_sentencepiece-vocab.json',\n",
       " './very_small_sentencepiece-merges.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencepiece_tokenizer.save_model(\n",
    "    directory = './',\n",
    "    prefix = 'very_small_sentencepiece'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Google SentencePiece Tokenizer\n",
    "\n",
    "- NAVER Movie rating data 를 이용한 sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "DATA_TRAIN_PATH = tf.keras.utils.get_file(\"ratings_train.txt\", \n",
    "        \"https://github.com/ironmanciti/NLP_lecture/raw/master/data/naver_movie/ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas.read_csv에서 quoting = 3으로 설정해주면 인용구(따옴표)를 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_TRAIN_PATH, sep='\\t', quoting=3)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149995, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위해 text 를 따로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./nsmc_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in data.document.values:\n",
    "        try:\n",
    "            f.write(line + '\\n')\n",
    "        except:\n",
    "            print(\"write error ---> \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149996\n",
      "아 더빙.. 진짜 짜증나네요 목소리\n"
     ]
    }
   ],
   "source": [
    "#write 가 잘 되었는지 확인\n",
    "with open('./nsmc_text.txt', 'r', encoding='utf-8') as f:\n",
    "    nsmc_txt = f.read().split('\\n')\n",
    "    \n",
    "print(len(nsmc_txt))\n",
    "print(nsmc_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--input=./nsmc_text.txt --model_prefix=./nsmc_sentencepiece/nsmc --vocab_size=30000'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = './nsmc_text.txt'\n",
    "vocab_size = 30000\n",
    "prefix = './nsmc_sentencepiece/nsmc'\n",
    "\n",
    "templates = '--input={} --model_prefix={} --vocab_size={}'\n",
    "cmd = templates.format(input_file, prefix, vocab_size)\n",
    "cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 806 ms, total: 1min 9s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n",
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "[52, 752, 5, 25, 16031, 1401] \n",
      "\n",
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "['▁흠', '...', '포스터보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "[1243, 6, 12798, 18636, 406, 47, 18166, 397, 1131, 6426, 1079, 411] \n",
      "\n",
      "너무재밓었다그래서보는것을추천한다\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는것', '을', '추천', '한다']\n",
      "[17, 600, 21563, 629, 2751, 10753, 14, 2317, 295] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in data.document.values[:3]:\n",
    "    print(t)\n",
    "    print(sp.encode_as_pieces(t))\n",
    "    print(sp.encode_as_ids(t), '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울이 되어서 날씨가 무척 추워요.\n",
      "['▁겨울', '이', '▁되어서', '▁날씨', '가', '▁무척', '▁추', '워', '요', '.']\n",
      "[5441, 10, 7683, 23623, 12, 2404, 2450, 814, 67, 3]\n",
      "\n",
      "이번 성탄절은 화이트 크리스마스가 될까요?\n",
      "['▁이번', '▁성', '탄', '절', '은', '▁화이트', '▁크리스마스', '가', '▁될까', '요', '?']\n",
      "[1623, 1119, 936, 1417, 18, 15787, 3080, 12, 10720, 67, 15]\n",
      "\n",
      "아버지가방에들어가신다\n",
      "['▁아버지가', '방', '에', '들어가', '신', '다']\n",
      "[6165, 628, 16, 13129, 271, 23]\n",
      "\n",
      "아버지가 방에 들어가신다\n",
      "['▁아버지가', '▁방', '에', '▁들어가', '신', '다']\n",
      "[6165, 1565, 16, 3875, 271, 23]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "  \"겨울이 되어서 날씨가 무척 추워요.\",\n",
    "  \"이번 성탄절은 화이트 크리스마스가 될까요?\",\n",
    "  '아버지가방에들어가신다',\n",
    "  '아버지가 방에 들어가신다'\n",
    "]\n",
    "for line in lines:\n",
    "  pieces = sp.encode_as_pieces(line)\n",
    "  ids = sp.encode_as_ids(line)\n",
    "  print(line)\n",
    "  print(pieces)\n",
    "  print(ids)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "001_Inspect_BERT_Vocabulary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
