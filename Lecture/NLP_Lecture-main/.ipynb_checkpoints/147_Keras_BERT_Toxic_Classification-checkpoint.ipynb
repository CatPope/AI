{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-EZfke1vUYq"
   },
   "source": [
    "# Simple Text Classification using BERT and Keras\n",
    "\n",
    "- toxic  \n",
    "- severe_toxic  \n",
    "- obscene  \n",
    "- threat  \n",
    "- insult  \n",
    "- identity_hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gBxaZJPvpmw"
   },
   "source": [
    "- install BERT tokenizer from the BERT python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EK3krsA8KRIp"
   },
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LNI7arEPIPx"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "09Ox_h10OhGx",
    "outputId": "d5b106f1-b5f0-4b93-eea1-7d58a9293067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n",
      "Hub version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0_fXgTiv62f"
   },
   "source": [
    "## BERT Embedding Layer\n",
    "\n",
    "- tf.hub 의 pre-trained parameter 로 BERT model 초기화 하고, downstream task 이 labeled data 로 모든 parameter 를 fine-tunning 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naYZF06_xbXT"
   },
   "source": [
    "BERT model 은 3 가지의 input token 을 필요로 한다.\n",
    "\n",
    "- input_word_ids : vocaburary 를 이용하여 input token 을 index 로 convert  \n",
    "- input_mask : 0 for padding. 1 for valid tokens  \n",
    "- segment_ids : 0 는 1st segment, 1 은 2nd segment\n",
    "\n",
    "pooped_out 은 전체 input sequence 를 표현하고, sequence_output 은 context 의 각 input token 을 표현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oMOOiIVmP-3z",
    "outputId": "4bc968e9-bdc1-4e2b-b9c8-444e480e1973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"keras_layer_1/Identity:0\", shape=(None, 768), dtype=float32)\n",
      "Tensor(\"keras_layer_1/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
    "\n",
    "MAX_SEQ_LEN=128\n",
    "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32)\n",
    "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32)\n",
    "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32)\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "print(pooled_output)\n",
    "print(sequence_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLiB9XbNyYz6"
   },
   "source": [
    "## Model creation \n",
    "pre-trained model 에 simple classification layer 를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYcDM53V05P"
   },
   "outputs": [],
   "source": [
    "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "out = tf.keras.layers.Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "e6CHF8WfV5gg",
    "outputId": "22f9e38e-f38b-4b50-f695-797ed56a78a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            4614        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,855\n",
      "Trainable params: 109,486,854\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WETBH8nvy-bf"
   },
   "source": [
    "## Tokenization\n",
    "- BERT 는 30,000 개의 token vocabulary 를 가진 WordPiece Embedding 사용  \n",
    "- original vocab file 을 이용하여 tokenizer import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQbqZQhXUTkE"
   },
   "outputs": [],
   "source": [
    "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer=FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9IaN5_Fkt7o"
   },
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BsauL7dkt_9"
   },
   "outputs": [],
   "source": [
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T88xysGX3uRV"
   },
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrE6fz8oOhOq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"trimurt001\" # username from the json file\n",
    "os.environ['KAGGLE_KEY'] = \"b8265fc98880a5f19a51ac0bf2573552\" # key from the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "uMHNlCXvlHaP",
    "outputId": "ac794d6f-f16e-46d8-d658-c381c4cebc70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kaggle'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_4MFikjLOhW1",
    "outputId": "706fafbe-d809-4a9a-ffdc-bef638e1f9f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!unzip train.csv.zip\n",
    "!unzip test.csv.zip\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hao1rgFmOhT6",
    "outputId": "bd869f13-a6d9-4a81-aff7-bd7b02e2f11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\t\t   test.csv\t test_labels.csv.zip  train.csv.zip\n",
      "sample_submission.csv.zip  test.csv.zip  train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGjo2CN43JxY"
   },
   "outputs": [],
   "source": [
    "df.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Vn_0yNpsOhZz",
    "outputId": "6f0b879a-6138-4890-d515-f5955978d4b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127382</th>\n",
       "      <td>a944167489a27b8f</td>\n",
       "      <td>\"\\n\\nThis is taken from the article cited: \"\"T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33821</th>\n",
       "      <td>5a32893b83d5ba66</td>\n",
       "      <td>You have only accused me here and no explanati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82639</th>\n",
       "      <td>dd0e1bed2a93f115</td>\n",
       "      <td>WP:SOFIXIT.\\nTake care, however, not to remove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117196</th>\n",
       "      <td>7245807e1ae3b573</td>\n",
       "      <td>Why was deMause's comment copied into the arti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44465</th>\n",
       "      <td>76bee0f6c212f29c</td>\n",
       "      <td>\"\\n\\nI'm shocked at your \"\"holier than thou\"\" ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  ... identity_hate\n",
       "127382  a944167489a27b8f  ...             0\n",
       "33821   5a32893b83d5ba66  ...             0\n",
       "82639   dd0e1bed2a93f115  ...             0\n",
       "117196  7245807e1ae3b573  ...             0\n",
       "44465   76bee0f6c212f29c  ...             0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('train.csv')\n",
    "\n",
    "df = df.sample(frac=1)   # frac = 1 : 100%\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6hQ0xkqOhcN"
   },
   "outputs": [],
   "source": [
    "train_sentences = df[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = df[list_classes].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ag3GN_g_4RS4"
   },
   "source": [
    "- 각 sequence 의 1st token 은 언제나 [CLS] special classification token 이다. [SEP] 은 special separator token 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2Z-ZcEhsECa"
   },
   "outputs": [],
   "source": [
    "def create_single_input(sentence, MAX_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    "\n",
    "  return ids,masks,segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnX-lYMrOheq"
   },
   "outputs": [],
   "source": [
    "def create_input_array(sentences):\n",
    "\n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    "\n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    "\n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4h8i-p55JaT"
   },
   "source": [
    "bert_layer 의 sequence_output 을 AveragePooling layer 에 통과사키고 6 unit 의 output layer 에 통과시켜 6 class classification 을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OenQWBJEvx9F",
    "outputId": "535ffad5-15a0-4452-8585-2c7b0d1b942b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [02:49<00:00, 944.16it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = create_input_array(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kZT3munMvw44",
    "outputId": "7f256943-5405-47c3-bfac-4394f270a6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3990/3990 [==============================] - 1911s 479ms/step - loss: 0.1475 - accuracy: 0.8883 - val_loss: 0.1397 - val_accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f328e8e6240>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputs, train_y, epochs=1, batch_size=32, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMbUMq365zQ8"
   },
   "source": [
    "## Predict\n",
    "- new text data 를 predict 하기 위해 BERT input 으로 변환 후 predict() method 로 predict\n",
    "\n",
    "- [toxic, severe_toxic, obscene, threat, insult, identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "coUa-kxPoskX",
    "outputId": "c9b8e484-6ab9-41f1-8350-5f6fd45d8f11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 741.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08126447 0.00891771 0.05784464 0.00303013 0.04035437 0.00531961]\n"
     ]
    }
   ],
   "source": [
    "test_df=pd.read_csv(\"test.csv\")\n",
    "\n",
    "test_sentences = test_df[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "test_inputs=create_input_array(test_sentences[110:150])\n",
    "\n",
    "print(model.predict(test_inputs)[0])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Keras BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
