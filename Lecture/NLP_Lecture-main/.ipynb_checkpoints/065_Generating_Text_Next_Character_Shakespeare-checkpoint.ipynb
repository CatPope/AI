{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PAQCwU1OPiO"
   },
   "source": [
    "# 065. next character 를 예측하여 Shakespeare Sonnet Text 생성\n",
    "\n",
    "- 순환 신경망을 활용한 문자열 생성\n",
    "\n",
    "\n",
    "\n",
    "- character 단위로 text 생성  \n",
    "\n",
    "    - 문자 시퀀스 (ex. \"Shakespear\")가 주어지면, 시퀀스의 다음 문자(\"e\")를 예측하는 모델을 훈련\n",
    "    - 모델을 반복하여 호출하면 더 긴 텍스트 시퀀스 생성이 가능\n",
    "    - 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르거나 심지어 텍스트의 단위가 단어라는 것도 모름 \n",
    "    \n",
    "    \n",
    "- google NLP tutorial 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fmAhbVhsOPiR",
    "outputId": "a07e66c7-552d-460a-a1a5-e4cad393d7d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3jcUzo1OPiU"
   },
   "source": [
    "# Text File download\n",
    "\n",
    "- 셰익스피어 데이터셋 다운로드  \n",
    "\n",
    "\n",
    "- data 는 ~/.keras/dataset 에 저장. absolute path 지정해 주어야 replace  됨. 다시 download 받을 경우 ~/.keras file 삭제 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "JwEh3FwEOPiV",
    "outputId": "001dc817-532a-453b-d74f-7775d4dbfdfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/ironmanciti/NLP_Lecture/raw/master/data/Alice_wonderland_Korean.txt\n",
      "147456/144596 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \n",
    "#                           \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "\n",
    "#path_to_file = tf.keras.utils.get_file(\"young-prince.txt\", \n",
    "#             \"https://raw.githubusercontent.com/ironmanciti/NLP_Lecture/master/data/young_prince.txt\")\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file(\"Alice_wonderland_Korean.txt\", \n",
    "                \"https://github.com/ironmanciti/NLP_Lecture/raw/master/data/Alice_wonderland_Korean.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PcnvJK5IOPiX",
    "outputId": "9b01adb1-dab8-43be-a2b5-55bfb91c1bea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84379"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = open(path_to_file, 'r').read()\n",
    "\n",
    "text = open(path_to_file,\"r\", encoding='cp949').read()   # Alice_wonderland encoding\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lbAD6rxdOPiZ",
    "outputId": "28d57b7c-9628-45d5-a6b9-70f11c31732e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앨리스는 언니 옆에 앉아 할일 없이 강둑에 앉아있는 게 지루해지기 시작했어요.\n",
      "그러다 한두 번 언니가 읽고 있는 책에 눈길을 주기도 했지만 그림이나 대화 하나 없는 책이지 뭐예요.\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNDT5LCkOPib"
   },
   "source": [
    "# lookup table 작성\n",
    "\n",
    "- 문자들을 숫자로 변환\n",
    "\n",
    "\n",
    "- 두 개의 조회 테이블(lookup table) 작성  \n",
    "    - character to index\n",
    "    - index to character\n",
    "\n",
    "\n",
    "- text 중에 포함된 character 들을 이용하여 charactet-to-index, index-to-character 변환 table 작성\n",
    "    - 하나는 문자를 숫자에 매핑하고 다른 하나는 숫자를 문자에 매핑  \n",
    "    - 문자를 0번 인덱스부터 고유 문자 길이까지 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGRWLMgLOPic"
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "nb_chars = len(chars)\n",
    "\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "wGC50ceCOPie",
    "outputId": "07dbc181-5e09-434e-b749-13eea901f214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 갯수 :  1093\n",
      "\n",
      "character to index lookup table : \n",
      " {'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, '+': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, '=': 21, '?': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'I': 27, 'J': 28, 'K': 29, 'M': 30, 'Q': 31, 'R': 32, 'X': 33, 'Y': 34, '[': 35, ']': 36, 'a': 37, 'b': 38, 'c': 39, 'd': 40, 'e': 41, 'f': 42, 'g': 43, 'h': 44, 'i': 45, 'k': 46, 'l': 47, 'm': 48, 'n': 49, 'o': 50, 'p': 51, 'r': 52, 's': 53, 't': 54, 'u': 55, 'v': 56, 'w': 57, 'x': 58, 'y': 59, 'z': 60, '~': 61, 'ː': 62, '―': 63, '‘': 64, '’': 65, '“': 66, '”': 67, '…': 68, '←': 69, '→': 70, '▶': 71, '◆': 72, '○': 73, '♥': 74, 'ㄱ': 75, 'ㄴ': 76, 'ㄹ': 77, 'ㅀ': 78, 'ㅁ': 79, 'ㅂ': 80, 'ㅆ': 81, 'ㅈ': 82, 'ㅉ': 83, 'ㅋ': 84, 'ㅍ': 85, 'ㅛ': 86, 'ㅝ': 87, 'ㅡ': 88, 'ㅣ': 89, 'ㅤ': 90, '가': 91, '각': 92, '간': 93, '갇': 94, '갈': 95, '감': 96, '갑': 97, '값': 98, '갔': 99, '강': 100, '갖': 101, '같': 102, '갛': 103, '개': 104, '객': 105, '갠': 106, '갯': 107, '걀': 108, '거': 109, '걱': 110, '건': 111, '걷': 112, '걸': 113, '검': 114, '겁': 115, '것': 116, '겅': 117, '게': 118, '겐': 119, '겠': 120, '겨': 121, '격': 122, '겪': 123, '견': 124, '결': 125, '겸': 126, '겹': 127, '겼': 128, '경': 129, '곁': 130, '계': 131, '고': 132, '곡': 133, '곤': 134, '곧': 135, '골': 136, '곯': 137, '곰': 138, '곱': 139, '곳': 140, '공': 141, '과': 142, '관': 143, '괄': 144, '광': 145, '괜': 146, '괴': 147, '굉': 148, '교': 149, '구': 150, '국': 151, '군': 152, '굳': 153, '굴': 154, '굽': 155, '궁': 156, '권': 157, '궐': 158, '궤': 159, '귀': 160, '귓': 161, '규': 162, '그': 163, '극': 164, '근': 165, '글': 166, '긁': 167, '금': 168, '급': 169, '긋': 170, '기': 171, '긴': 172, '길': 173, '김': 174, '깃': 175, '깊': 176, '까': 177, '깍': 178, '깐': 179, '깔': 180, '깜': 181, '깝': 182, '깥': 183, '깨': 184, '깬': 185, '깰': 186, '깽': 187, '꺼': 188, '꺽': 189, '껄': 190, '껍': 191, '껏': 192, '껑': 193, '께': 194, '껴': 195, '꼈': 196, '꼬': 197, '꼭': 198, '꼴': 199, '꼼': 200, '꽃': 201, '꽉': 202, '꽤': 203, '꾸': 204, '꾹': 205, '꾼': 206, '꿀': 207, '꿇': 208, '꿈': 209, '꿍': 210, '꿔': 211, '꿨': 212, '뀌': 213, '뀐': 214, '끄': 215, '끈': 216, '끊': 217, '끌': 218, '끔': 219, '끗': 220, '끝': 221, '끼': 222, '끽': 223, '낀': 224, '낄': 225, '낌': 226, '나': 227, '낙': 228, '낚': 229, '난': 230, '날': 231, '남': 232, '납': 233, '났': 234, '낭': 235, '낮': 236, '낯': 237, '낳': 238, '내': 239, '낸': 240, '낼': 241, '냄': 242, '냅': 243, '냈': 244, '냉': 245, '냐': 246, '냘': 247, '냥': 248, '너': 249, '넋': 250, '넌': 251, '널': 252, '넘': 253, '넣': 254, '네': 255, '넨': 256, '녀': 257, '녁': 258, '년': 259, '념': 260, '녔': 261, '녕': 262, '노': 263, '논': 264, '놀': 265, '놈': 266, '놋': 267, '농': 268, '높': 269, '놓': 270, '뇨': 271, '누': 272, '눅': 273, '눈': 274, '눌': 275, '눕': 276, '눠': 277, '뉴': 278, '늉': 279, '느': 280, '는': 281, '늘': 282, '늙': 283, '능': 284, '늦': 285, '니': 286, '닌': 287, '닐': 288, '님': 289, '닙': 290, '다': 291, '닥': 292, '닦': 293, '단': 294, '닫': 295, '달': 296, '닭': 297, '닮': 298, '담': 299, '답': 300, '닷': 301, '당': 302, '닿': 303, '대': 304, '댄': 305, '댔': 306, '더': 307, '던': 308, '덜': 309, '덥': 310, '덧': 311, '덩': 312, '덫': 313, '덮': 314, '데': 315, '뎁': 316, '도': 317, '독': 318, '돌': 319, '동': 320, '돼': 321, '됐': 322, '되': 323, '된': 324, '될': 325, '됨': 326, '두': 327, '둑': 328, '둔': 329, '둘': 330, '둠': 331, '둥': 332, '뒤': 333, '뒷': 334, '드': 335, '득': 336, '든': 337, '듣': 338, '들': 339, '듦': 340, '듬': 341, '듭': 342, '듯': 343, '등': 344, '디': 345, '딜': 346, '딨': 347, '딪': 348, '따': 349, '딱': 350, '딴': 351, '딸': 352, '땅': 353, '때': 354, '땐': 355, '땜': 356, '땡': 357, '떠': 358, '떡': 359, '떤': 360, '떨': 361, '떴': 362, '떻': 363, '떼': 364, '또': 365, '똑': 366, '똘': 367, '똥': 368, '뚜': 369, '뚝': 370, '뚱': 371, '뛰': 372, '뜀': 373, '뜨': 374, '뜩': 375, '뜬': 376, '뜯': 377, '뜸': 378, '뜻': 379, '띄': 380, '띠': 381, '띤': 382, '라': 383, '락': 384, '란': 385, '랄': 386, '람': 387, '랍': 388, '랐': 389, '랑': 390, '랗': 391, '래': 392, '랜': 393, '램': 394, '랩': 395, '랫': 396, '랬': 397, '러': 398, '럭': 399, '런': 400, '럴': 401, '럼': 402, '럽': 403, '렀': 404, '렁': 405, '렇': 406, '레': 407, '렌': 408, '려': 409, '력': 410, '련': 411, '렬': 412, '렴': 413, '렵': 414, '렸': 415, '령': 416, '례': 417, '로': 418, '록': 419, '론': 420, '롭': 421, '롱': 422, '료': 423, '루': 424, '룬': 425, '룻': 426, '류': 427, '르': 428, '른': 429, '를': 430, '름': 431, '릅': 432, '릇': 433, '릎': 434, '리': 435, '릭': 436, '린': 437, '릴': 438, '림': 439, '립': 440, '릿': 441, '링': 442, '마': 443, '막': 444, '만': 445, '많': 446, '말': 447, '맑': 448, '맘': 449, '맙': 450, '맛': 451, '망': 452, '맞': 453, '맡': 454, '맣': 455, '매': 456, '맹': 457, '맺': 458, '머': 459, '먹': 460, '먼': 461, '멀': 462, '멈': 463, '멋': 464, '멍': 465, '메': 466, '며': 467, '면': 468, '멸': 469, '명': 470, '몇': 471, '모': 472, '목': 473, '몫': 474, '몬': 475, '몰': 476, '몸': 477, '못': 478, '몽': 479, '묘': 480, '무': 481, '묵': 482, '묶': 483, '문': 484, '묻': 485, '물': 486, '뭇': 487, '뭍': 488, '뭐': 489, '뭔': 490, '뭘': 491, '뭣': 492, '미': 493, '민': 494, '믿': 495, '밀': 496, '밈': 497, '밌': 498, '밑': 499, '바': 500, '박': 501, '밖': 502, '반': 503, '받': 504, '발': 505, '밝': 506, '밟': 507, '밤': 508, '밥': 509, '방': 510, '밭': 511, '배': 512, '백': 513, '뱀': 514, '뱃': 515, '뱉': 516, '버': 517, '벅': 518, '번': 519, '벌': 520, '범': 521, '법': 522, '벗': 523, '벙': 524, '베': 525, '벨': 526, '벳': 527, '벽': 528, '변': 529, '별': 530, '볍': 531, '볐': 532, '병': 533, '볕': 534, '보': 535, '복': 536, '볶': 537, '본': 538, '볼': 539, '봉': 540, '봐': 541, '봤': 542, '부': 543, '북': 544, '분': 545, '불': 546, '붉': 547, '붐': 548, '붓': 549, '붕': 550, '붙': 551, '브': 552, '블': 553, '비': 554, '빌': 555, '빗': 556, '빙': 557, '빛': 558, '빠': 559, '빡': 560, '빤': 561, '빨': 562, '빴': 563, '빵': 564, '빼': 565, '뺐': 566, '뺨': 567, '뻐': 568, '뻔': 569, '뻗': 570, '뻘': 571, '뻤': 572, '뼈': 573, '뼛': 574, '뽀': 575, '뽐': 576, '뽑': 577, '뿌': 578, '뿐': 579, '뿔': 580, '쁘': 581, '쁜': 582, '쁠': 583, '쁨': 584, '삐': 585, '사': 586, '삭': 587, '산': 588, '살': 589, '삶': 590, '삼': 591, '삽': 592, '상': 593, '샅': 594, '새': 595, '색': 596, '샘': 597, '생': 598, '서': 599, '석': 600, '섞': 601, '선': 602, '설': 603, '섬': 604, '섭': 605, '섯': 606, '섰': 607, '성': 608, '세': 609, '센': 610, '셈': 611, '셋': 612, '셔': 613, '셕': 614, '션': 615, '셨': 616, '셰': 617, '소': 618, '속': 619, '손': 620, '솔': 621, '솟': 622, '송': 623, '솥': 624, '쇠': 625, '쇼': 626, '수': 627, '숙': 628, '순': 629, '숟': 630, '술': 631, '숨': 632, '숫': 633, '숲': 634, '쉬': 635, '쉰': 636, '쉴': 637, '쉽': 638, '쉿': 639, '스': 640, '슨': 641, '슬': 642, '슴': 643, '습': 644, '슷': 645, '승': 646, '시': 647, '식': 648, '신': 649, '실': 650, '싫': 651, '심': 652, '십': 653, '싱': 654, '싶': 655, '싸': 656, '싹': 657, '싼': 658, '쌌': 659, '쌍': 660, '쌓': 661, '쌔': 662, '써': 663, '썩': 664, '썼': 665, '썽': 666, '쎄': 667, '쏙': 668, '쏟': 669, '쑥': 670, '쓰': 671, '쓴': 672, '쓸': 673, '씀': 674, '씌': 675, '씨': 676, '씩': 677, '씬': 678, '씻': 679, '씽': 680, '아': 681, '악': 682, '안': 683, '앉': 684, '않': 685, '알': 686, '암': 687, '압': 688, '앗': 689, '았': 690, '앙': 691, '앞': 692, '애': 693, '액': 694, '앤': 695, '앨': 696, '앵': 697, '야': 698, '약': 699, '얀': 700, '얇': 701, '양': 702, '얕': 703, '얘': 704, '얜': 705, '어': 706, '억': 707, '언': 708, '얹': 709, '얻': 710, '얼': 711, '엄': 712, '업': 713, '없': 714, '엇': 715, '었': 716, '엉': 717, '엌': 718, '엎': 719, '에': 720, '엔': 721, '엘': 722, '여': 723, '역': 724, '연': 725, '열': 726, '염': 727, '엽': 728, '엾': 729, '엿': 730, '였': 731, '영': 732, '옆': 733, '예': 734, '옛': 735, '오': 736, '옥': 737, '온': 738, '올': 739, '옮': 740, '옳': 741, '옴': 742, '옵': 743, '옷': 744, '옹': 745, '와': 746, '왁': 747, '완': 748, '왈': 749, '왔': 750, '왕': 751, '왜': 752, '외': 753, '왼': 754, '요': 755, '욕': 756, '욘': 757, '용': 758, '우': 759, '운': 760, '울': 761, '움': 762, '웃': 763, '웅': 764, '워': 765, '원': 766, '월': 767, '웠': 768, '웩': 769, '웬': 770, '위': 771, '윈': 772, '윌': 773, '윙': 774, '유': 775, '육': 776, '으': 777, '은': 778, '을': 779, '읊': 780, '음': 781, '응': 782, '의': 783, '이': 784, '익': 785, '인': 786, '일': 787, '읽': 788, '잃': 789, '임': 790, '입': 791, '있': 792, '잉': 793, '잊': 794, '잎': 795, '자': 796, '작': 797, '잔': 798, '잖': 799, '잘': 800, '잠': 801, '잡': 802, '잣': 803, '잤': 804, '장': 805, '재': 806, '잭': 807, '잼': 808, '잽': 809, '쟁': 810, '저': 811, '적': 812, '전': 813, '절': 814, '젊': 815, '점': 816, '접': 817, '젓': 818, '정': 819, '젖': 820, '제': 821, '젠': 822, '져': 823, '졌': 824, '조': 825, '족': 826, '존': 827, '졸': 828, '좀': 829, '좁': 830, '종': 831, '좋': 832, '좌': 833, '죄': 834, '죠': 835, '주': 836, '죽': 837, '준': 838, '줄': 839, '줍': 840, '중': 841, '줘': 842, '줬': 843, '쥐': 844, '즈': 845, '즉': 846, '즐': 847, '증': 848, '지': 849, '직': 850, '진': 851, '질': 852, '짐': 853, '집': 854, '짓': 855, '짖': 856, '짜': 857, '짝': 858, '짧': 859, '짱': 860, '째': 861, '쨌': 862, '쨍': 863, '쩌': 864, '쩍': 865, '쩐': 866, '쩔': 867, '쩜': 868, '쪼': 869, '쪽': 870, '쫓': 871, '쭉': 872, '쯤': 873, '쯧': 874, '찌': 875, '찍': 876, '찔': 877, '찝': 878, '찢': 879, '차': 880, '착': 881, '찬': 882, '찮': 883, '찰': 884, '참': 885, '찻': 886, '창': 887, '찾': 888, '채': 889, '책': 890, '챘': 891, '처': 892, '척': 893, '천': 894, '철': 895, '첨': 896, '첫': 897, '청': 898, '체': 899, '쳐': 900, '쳤': 901, '초': 902, '촉': 903, '총': 904, '최': 905, '쵸': 906, '추': 907, '축': 908, '춘': 909, '출': 910, '춤': 911, '춧': 912, '충': 913, '춰': 914, '췄': 915, '취': 916, '츠': 917, '측': 918, '층': 919, '치': 920, '칙': 921, '친': 922, '칠': 923, '침': 924, '칭': 925, '카': 926, '칵': 927, '칼': 928, '캐': 929, '캔': 930, '캥': 931, '커': 932, '컥': 933, '컸': 934, '케': 935, '켁': 936, '켓': 937, '켜': 938, '켤': 939, '켰': 940, '코': 941, '콘': 942, '콤': 943, '콧': 944, '쾌': 945, '쿠': 946, '쿨': 947, '쿵': 948, '퀴': 949, '크': 950, '큰': 951, '큼': 952, '키': 953, '킥': 954, '킨': 955, '킬': 956, '킹': 957, '타': 958, '탁': 959, '탄': 960, '탈': 961, '탓': 962, '탕': 963, '태': 964, '택': 965, '탠': 966, '탤': 967, '탱': 968, '터': 969, '턱': 970, '턴': 971, '털': 972, '텅': 973, '테': 974, '텐': 975, '템': 976, '텝': 977, '토': 978, '톤': 979, '톱': 980, '통': 981, '투': 982, '툴': 983, '툼': 984, '퉁': 985, '튀': 986, '튤': 987, '트': 988, '특': 989, '튼': 990, '틀': 991, '틈': 992, '티': 993, '틴': 994, '틸': 995, '파': 996, '판': 997, '팔': 998, '팠': 999, '패': 1000, '팬': 1001, '팽': 1002, '퍼': 1003, '펄': 1004, '펐': 1005, '펑': 1006, '페': 1007, '펜': 1008, '펫': 1009, '펴': 1010, '편': 1011, '펼': 1012, '평': 1013, '폐': 1014, '포': 1015, '폭': 1016, '폴': 1017, '표': 1018, '푸': 1019, '푹': 1020, '푼': 1021, '풀': 1022, '품': 1023, '풋': 1024, '풍': 1025, '프': 1026, '픈': 1027, '플': 1028, '픔': 1029, '피': 1030, '핀': 1031, '필': 1032, '핑': 1033, '하': 1034, '학': 1035, '한': 1036, '할': 1037, '핥': 1038, '함': 1039, '합': 1040, '항': 1041, '해': 1042, '햇': 1043, '했': 1044, '행': 1045, '향': 1046, '허': 1047, '헐': 1048, '험': 1049, '헤': 1050, '헴': 1051, '헷': 1052, '혀': 1053, '현': 1054, '혔': 1055, '형': 1056, '호': 1057, '혹': 1058, '혼': 1059, '홀': 1060, '홍': 1061, '화': 1062, '확': 1063, '환': 1064, '활': 1065, '황': 1066, '회': 1067, '획': 1068, '효': 1069, '후': 1070, '훈': 1071, '훔': 1072, '훤': 1073, '훨': 1074, '휘': 1075, '휜': 1076, '휩': 1077, '휴': 1078, '흉': 1079, '흐': 1080, '흔': 1081, '흘': 1082, '흠': 1083, '흡': 1084, '흥': 1085, '흩': 1086, '희': 1087, '흰': 1088, '히': 1089, '힌': 1090, '힐': 1091, '힘': 1092}\n",
      "\n",
      "index to character lookup table : \n",
      " ['\\n', ' ', '!', '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '=', '?', 'C', 'D', 'E', 'F', 'I', 'J', 'K', 'M', 'Q', 'R', 'X', 'Y', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', 'ː', '―', '‘', '’', '“', '”', '…', '←', '→', '▶', '◆', '○', '♥', 'ㄱ', 'ㄴ', 'ㄹ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅆ', 'ㅈ', 'ㅉ', 'ㅋ', 'ㅍ', 'ㅛ', 'ㅝ', 'ㅡ', 'ㅣ', 'ㅤ', '가', '각', '간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '갛', '개', '객', '갠', '갯', '걀', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '겅', '게', '겐', '겠', '겨', '격', '겪', '견', '결', '겸', '겹', '겼', '경', '곁', '계', '고', '곡', '곤', '곧', '골', '곯', '곰', '곱', '곳', '공', '과', '관', '괄', '광', '괜', '괴', '굉', '교', '구', '국', '군', '굳', '굴', '굽', '궁', '권', '궐', '궤', '귀', '귓', '규', '그', '극', '근', '글', '긁', '금', '급', '긋', '기', '긴', '길', '김', '깃', '깊', '까', '깍', '깐', '깔', '깜', '깝', '깥', '깨', '깬', '깰', '깽', '꺼', '꺽', '껄', '껍', '껏', '껑', '께', '껴', '꼈', '꼬', '꼭', '꼴', '꼼', '꽃', '꽉', '꽤', '꾸', '꾹', '꾼', '꿀', '꿇', '꿈', '꿍', '꿔', '꿨', '뀌', '뀐', '끄', '끈', '끊', '끌', '끔', '끗', '끝', '끼', '끽', '낀', '낄', '낌', '나', '낙', '낚', '난', '날', '남', '납', '났', '낭', '낮', '낯', '낳', '내', '낸', '낼', '냄', '냅', '냈', '냉', '냐', '냘', '냥', '너', '넋', '넌', '널', '넘', '넣', '네', '넨', '녀', '녁', '년', '념', '녔', '녕', '노', '논', '놀', '놈', '놋', '농', '높', '놓', '뇨', '누', '눅', '눈', '눌', '눕', '눠', '뉴', '늉', '느', '는', '늘', '늙', '능', '늦', '니', '닌', '닐', '님', '닙', '다', '닥', '닦', '단', '닫', '달', '닭', '닮', '담', '답', '닷', '당', '닿', '대', '댄', '댔', '더', '던', '덜', '덥', '덧', '덩', '덫', '덮', '데', '뎁', '도', '독', '돌', '동', '돼', '됐', '되', '된', '될', '됨', '두', '둑', '둔', '둘', '둠', '둥', '뒤', '뒷', '드', '득', '든', '듣', '들', '듦', '듬', '듭', '듯', '등', '디', '딜', '딨', '딪', '따', '딱', '딴', '딸', '땅', '때', '땐', '땜', '땡', '떠', '떡', '떤', '떨', '떴', '떻', '떼', '또', '똑', '똘', '똥', '뚜', '뚝', '뚱', '뛰', '뜀', '뜨', '뜩', '뜬', '뜯', '뜸', '뜻', '띄', '띠', '띤', '라', '락', '란', '랄', '람', '랍', '랐', '랑', '랗', '래', '랜', '램', '랩', '랫', '랬', '러', '럭', '런', '럴', '럼', '럽', '렀', '렁', '렇', '레', '렌', '려', '력', '련', '렬', '렴', '렵', '렸', '령', '례', '로', '록', '론', '롭', '롱', '료', '루', '룬', '룻', '류', '르', '른', '를', '름', '릅', '릇', '릎', '리', '릭', '린', '릴', '림', '립', '릿', '링', '마', '막', '만', '많', '말', '맑', '맘', '맙', '맛', '망', '맞', '맡', '맣', '매', '맹', '맺', '머', '먹', '먼', '멀', '멈', '멋', '멍', '메', '며', '면', '멸', '명', '몇', '모', '목', '몫', '몬', '몰', '몸', '못', '몽', '묘', '무', '묵', '묶', '문', '묻', '물', '뭇', '뭍', '뭐', '뭔', '뭘', '뭣', '미', '민', '믿', '밀', '밈', '밌', '밑', '바', '박', '밖', '반', '받', '발', '밝', '밟', '밤', '밥', '방', '밭', '배', '백', '뱀', '뱃', '뱉', '버', '벅', '번', '벌', '범', '법', '벗', '벙', '베', '벨', '벳', '벽', '변', '별', '볍', '볐', '병', '볕', '보', '복', '볶', '본', '볼', '봉', '봐', '봤', '부', '북', '분', '불', '붉', '붐', '붓', '붕', '붙', '브', '블', '비', '빌', '빗', '빙', '빛', '빠', '빡', '빤', '빨', '빴', '빵', '빼', '뺐', '뺨', '뻐', '뻔', '뻗', '뻘', '뻤', '뼈', '뼛', '뽀', '뽐', '뽑', '뿌', '뿐', '뿔', '쁘', '쁜', '쁠', '쁨', '삐', '사', '삭', '산', '살', '삶', '삼', '삽', '상', '샅', '새', '색', '샘', '생', '서', '석', '섞', '선', '설', '섬', '섭', '섯', '섰', '성', '세', '센', '셈', '셋', '셔', '셕', '션', '셨', '셰', '소', '속', '손', '솔', '솟', '송', '솥', '쇠', '쇼', '수', '숙', '순', '숟', '술', '숨', '숫', '숲', '쉬', '쉰', '쉴', '쉽', '쉿', '스', '슨', '슬', '슴', '습', '슷', '승', '시', '식', '신', '실', '싫', '심', '십', '싱', '싶', '싸', '싹', '싼', '쌌', '쌍', '쌓', '쌔', '써', '썩', '썼', '썽', '쎄', '쏙', '쏟', '쑥', '쓰', '쓴', '쓸', '씀', '씌', '씨', '씩', '씬', '씻', '씽', '아', '악', '안', '앉', '않', '알', '암', '압', '앗', '았', '앙', '앞', '애', '액', '앤', '앨', '앵', '야', '약', '얀', '얇', '양', '얕', '얘', '얜', '어', '억', '언', '얹', '얻', '얼', '엄', '업', '없', '엇', '었', '엉', '엌', '엎', '에', '엔', '엘', '여', '역', '연', '열', '염', '엽', '엾', '엿', '였', '영', '옆', '예', '옛', '오', '옥', '온', '올', '옮', '옳', '옴', '옵', '옷', '옹', '와', '왁', '완', '왈', '왔', '왕', '왜', '외', '왼', '요', '욕', '욘', '용', '우', '운', '울', '움', '웃', '웅', '워', '원', '월', '웠', '웩', '웬', '위', '윈', '윌', '윙', '유', '육', '으', '은', '을', '읊', '음', '응', '의', '이', '익', '인', '일', '읽', '잃', '임', '입', '있', '잉', '잊', '잎', '자', '작', '잔', '잖', '잘', '잠', '잡', '잣', '잤', '장', '재', '잭', '잼', '잽', '쟁', '저', '적', '전', '절', '젊', '점', '접', '젓', '정', '젖', '제', '젠', '져', '졌', '조', '족', '존', '졸', '좀', '좁', '종', '좋', '좌', '죄', '죠', '주', '죽', '준', '줄', '줍', '중', '줘', '줬', '쥐', '즈', '즉', '즐', '증', '지', '직', '진', '질', '짐', '집', '짓', '짖', '짜', '짝', '짧', '짱', '째', '쨌', '쨍', '쩌', '쩍', '쩐', '쩔', '쩜', '쪼', '쪽', '쫓', '쭉', '쯤', '쯧', '찌', '찍', '찔', '찝', '찢', '차', '착', '찬', '찮', '찰', '참', '찻', '창', '찾', '채', '책', '챘', '처', '척', '천', '철', '첨', '첫', '청', '체', '쳐', '쳤', '초', '촉', '총', '최', '쵸', '추', '축', '춘', '출', '춤', '춧', '충', '춰', '췄', '취', '츠', '측', '층', '치', '칙', '친', '칠', '침', '칭', '카', '칵', '칼', '캐', '캔', '캥', '커', '컥', '컸', '케', '켁', '켓', '켜', '켤', '켰', '코', '콘', '콤', '콧', '쾌', '쿠', '쿨', '쿵', '퀴', '크', '큰', '큼', '키', '킥', '킨', '킬', '킹', '타', '탁', '탄', '탈', '탓', '탕', '태', '택', '탠', '탤', '탱', '터', '턱', '턴', '털', '텅', '테', '텐', '템', '텝', '토', '톤', '톱', '통', '투', '툴', '툼', '퉁', '튀', '튤', '트', '특', '튼', '틀', '틈', '티', '틴', '틸', '파', '판', '팔', '팠', '패', '팬', '팽', '퍼', '펄', '펐', '펑', '페', '펜', '펫', '펴', '편', '펼', '평', '폐', '포', '폭', '폴', '표', '푸', '푹', '푼', '풀', '품', '풋', '풍', '프', '픈', '플', '픔', '피', '핀', '필', '핑', '하', '학', '한', '할', '핥', '함', '합', '항', '해', '햇', '했', '행', '향', '허', '헐', '험', '헤', '헴', '헷', '혀', '현', '혔', '형', '호', '혹', '혼', '홀', '홍', '화', '확', '환', '활', '황', '회', '획', '효', '후', '훈', '훔', '훤', '훨', '휘', '휜', '휩', '휴', '흉', '흐', '흔', '흘', '흠', '흡', '흥', '흩', '희', '흰', '히', '힌', '힐', '힘']\n"
     ]
    }
   ],
   "source": [
    "print(\"문자 갯수 : \", nb_chars)\n",
    "print()\n",
    "print(\"character to index lookup table : \\n\", char2idx)\n",
    "print()\n",
    "print(\"index to character lookup table : \\n\",idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0Xp9ZdjOPig"
   },
   "source": [
    "### 훈련 sample 과 target 만들기 \n",
    "\n",
    "- input data 를 고정길이로 통일하기 위해 MAX_SEQ_LEN 을 입력의 최대 길이로 지정 (임의로 결정)   \n",
    "\n",
    "\n",
    "- MAX_SEQ_LEN 개의 연속된 character sequence 를 input 으로 하고, 1 character shift 하여 이어지는 동일한 길이의 character sequence 를 label 로 만든다.  \n",
    "\n",
    "- 따라서, MAX_SEQ_LEN + 1 길이의 덩어리(chunk)로 text 를 분할하여 input, label 분리 (tf.Dataset method 사용)\n",
    "        ex) MAX_SEQ_LEN = 4 인 경우,\n",
    "            \"Hell\"  --> \"ello\"\n",
    "            \n",
    "\n",
    "- 이를 손쉽게 하기 위해 tensorflow 가 제공하는 tf.data.Dataset.from_tensor_slices 함수를 사용하여 text vector 를 character index 의 stream 으로 변환  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bcayD9YMOPig",
    "outputId": "7b1ac2b3-ae4d-4352-d38a-99db94d9b501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한 epoch 당 처리할 전체 sample 수 : 843\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 100         # time step = 100, 단일 입력에 대해 원하는 문장의 최대 글자수\n",
    "examples_per_epoch = len(text) // MAX_SEQ_LEN\n",
    "print(\"한 epoch 당 처리할 전체 sample 수 :\", examples_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtnZ_JzXbw8o"
   },
   "source": [
    "**char2index lookup table 을 이용하여 text 전체를 integer 로 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CtPLCd6mW_dU",
    "outputId": "0e1b7eee-427b-46ef-c7e3-c188e1300fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[696 435 640 ... 286 291   8]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lMkbCHj1OPii",
    "outputId": "46aaa3f2-6098-4a8d-8102-284be45008b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환전 : 앨리스는 언니 옆에 앉아, 변환후 : [696 435 640 281   1 708 286   1 733 720   1 684 681]\n"
     ]
    }
   ],
   "source": [
    "print(\"변환전 : {}, 변환후 : {}\".format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "585Hy1zFbw85"
   },
   "source": [
    "# 훈련 sample 및 target 만들기\n",
    "\n",
    "- tf.data.Dataset.from_tensor_slices method $\\rightarrow$ 주어진 tensor 들을 first dimension 을 따라 slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4p9oojdJOPik",
    "outputId": "6f4477ba-72d0-4ecd-cdcc-a115e546ae51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "T-4jpYOLOPil",
    "outputId": "91d6c64d-1631-417a-a9e8-6a29defcaa65",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[696, 435, 640, 281, 1]\n",
      "['앨', '리', '스', '는', ' ']\n"
     ]
    }
   ],
   "source": [
    "seq = [idx.numpy() for idx in char_dataset.take(5)]\n",
    "print(seq)\n",
    "print([''.join(idx2char[i]) for i in seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_S9tyTPOPin"
   },
   "source": [
    "  ## dataset 생성 - input text, target text\n",
    "\n",
    "**tf.data 의 batch method 를 사용하면 개별 character 를 원하는 size 의 character sequence 로 쉽게 바꿀 수 있다**\n",
    "\n",
    "- char_dataset 을 seq_len +1 길이의 character sequence 로 변환 (next character 를 예측하는 문제이므로 +1) 하여  \n",
    "  [ : sequence]는 input 으로 사용하고 [1 : sequence+1] 은 label 로 사용  \n",
    "  \n",
    "\n",
    "- sequnces 를 입력 text 와 target text 로 분리하는 helper 함수를 작성하여 map 함수로 각 sequences 의 element 에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kko0ZU25emvq"
   },
   "outputs": [],
   "source": [
    "# sequence 를 input, target 으로 분리\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "tlL1TeVqOPio",
    "outputId": "a2c8a40d-053b-4bc3-c098-daab5012bde8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[696, 435, 640, 281, 1, 708, 286, 1, 733, 720, 1, 684, 681, 1, 1037, 787, 1, 714, 784, 1, 100, 328, 720, 1, 684, 681, 792, 281, 1, 118, 1, 849, 424, 1042, 849, 171, 1, 647, 797, 1044, 706, 755, 8, 0, 163, 398, 291, 1, 1036, 327, 1, 519, 1, 708, 286, 91, 1, 788, 132, 1, 792, 281, 1, 890, 720, 1, 274, 173, 779, 1, 836, 171, 317, 1, 1044, 849, 445, 1, 163, 439, 784, 227, 1, 304, 1062, 1, 1034, 227, 1, 714, 281, 1, 890, 784, 849, 1, 489, 734, 755, 8]\n",
      "[435, 640, 281, 1, 708, 286, 1, 733, 720, 1, 684, 681, 1, 1037, 787, 1, 714, 784, 1, 100, 328, 720, 1, 684, 681, 792, 281, 1, 118, 1, 849, 424, 1042, 849, 171, 1, 647, 797, 1044, 706, 755, 8, 0, 163, 398, 291, 1, 1036, 327, 1, 519, 1, 708, 286, 91, 1, 788, 132, 1, 792, 281, 1, 890, 720, 1, 274, 173, 779, 1, 836, 171, 317, 1, 1044, 849, 445, 1, 163, 439, 784, 227, 1, 304, 1062, 1, 1034, 227, 1, 714, 281, 1, 890, 784, 849, 1, 489, 734, 755, 8, 0]\n",
      "입력 data   =  '앨리스는 언니 옆에 앉아 할일 없이 강둑에 앉아있는 게 지루해지기 시작했어요.\\n그러다 한두 번 언니가 읽고 있는 책에 눈길을 주기도 했지만 그림이나 대화 하나 없는 책이지 뭐예요.'\n",
      "출력 data   =  '리스는 언니 옆에 앉아 할일 없이 강둑에 앉아있는 게 지루해지기 시작했어요.\\n그러다 한두 번 언니가 읽고 있는 책에 눈길을 주기도 했지만 그림이나 대화 하나 없는 책이지 뭐예요.\\n'\n"
     ]
    }
   ],
   "source": [
    "dataset = char_dataset.batch(MAX_SEQ_LEN + 1, drop_remainder=True)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "input, target = next(iter(dataset))\n",
    "input_seq  = [i for i in input.numpy()]\n",
    "target_seq = [i for i in target.numpy()]\n",
    "print(input_seq)\n",
    "print(target_seq)\n",
    "print(\"입력 data   = \", repr(''.join(idx2char[idx] for idx in input_seq)))\n",
    "print(\"출력 data   = \", repr(''.join(idx2char[idx] for idx in target_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zh4A46JVcLPm"
   },
   "source": [
    "# Training / Target dataset 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Sfe5e8gOPiv"
   },
   "source": [
    "- input_sample 의 각 인덱스는 하나의 타임 스텝(time step)으로 처리  \n",
    "\n",
    " \n",
    "- 위의 경우, time step 0 의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측   \n",
    "\n",
    "\n",
    "- 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트(context)를 고려하여 예측  \n",
    "\n",
    "\n",
    "- RNN 의 context 정보는 hidden state 를 뜻함  \n",
    "\n",
    "\n",
    "- 이전 batch 의 context 정보를 다음 batch 의 initial hidden state 로 받기 위해 RNN model 정의 시 stateful=True 로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnYySGNgOPix"
   },
   "source": [
    "## Training batch 생성\n",
    "\n",
    "- tf.data를 사용하여 train batch 생성    \n",
    "\n",
    "\n",
    "- 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만든다 (다음 character 를 예측하는 모델이고 임의의 batch size 로 잘랐으므로 shuffle 필요)  \n",
    "\n",
    "\n",
    "- batch method 를 이용하여 train batch 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-MKPskONOPiy",
    "outputId": "de2cd90a-2c66-467e-c223-b3366ed1e361"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2svhl5JOPi0"
   },
   "source": [
    "## Model 생성\n",
    "\n",
    "- Many-to-Many type\n",
    "\n",
    "<div>\n",
    "<img src=\"https://tensorflow.org/tutorials/text/images/text_generation_training.png\", width=\"600\">\n",
    "</div>\n",
    "\n",
    "- tf.keras.Sequential model 사용  \n",
    "\n",
    "\n",
    "- 3 개의 층을 사용하여 모델 정의  \n",
    "\n",
    "\n",
    "    1. Embedding : 입력층, embedding_dim 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련가능한 검색 table\n",
    "        - 각 character 에 대해 model 은 embedding 을 검색하고, embedding 을 입력으로 하여 GRU(LSTM) 를 1 개의 time step 으로 실행  \n",
    "\n",
    "    2. LSTM (GRU) 를 이용한 RNN 층\n",
    "\n",
    "    3. 완전연결층(Dense) \n",
    "        - logits 생성 : (64, 100, 65) - (batch size, MAX_SEQ_LEN, vocab_size)\n",
    "\n",
    "\n",
    "- Dense layer 에서 생성된 크기가 vocab_size 인 logit 을 tf.random.categorical 에 적용하여 다음 문자의 확률분포 예측  \n",
    "\n",
    "\n",
    "- stateful=True 로 지정 - 문장이 계속 연속되므로 이전 batch 의 hidden state 를 다음번 batch 의 초기 state 로 사용  \n",
    "\n",
    "\n",
    "- batch_input_shape 반드시 지정 ($t_i$ 의 initial state 가 $t_{i+bs}$ 로 연결되므로)\n",
    "\n",
    "\n",
    "- 입력 batch shape : (64, 100)  \n",
    "- 출력 batch shape : (64, 100, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pefilXaIOPi0",
    "outputId": "1d38e053-1bc8-434b-d959-ffba28b19ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "print(nb_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr-mu5Bhbw9l"
   },
   "source": [
    "**마지막 Dense layer 의 activation='linear' --> shape(100, 65) 의 logit 생성**\n",
    "\n",
    "- last layer 의 activation 을 softmax 로 하는 것보다 계산상 안정적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-lXLCk7OPi2"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, \n",
    "                             return_sequences=True, \n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(nb_chars)   # linear activation (logit 생성)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=nb_chars, embedding_dim=EMBEDDING_DIM, rnn_units=RNN_UNITS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "7h0EPDQ5bw9s",
    "outputId": "21d6203f-1bbc-4611-a16a-a0d8c63184c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           279808    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 1093)          1120325   \n",
      "=================================================================\n",
      "Total params: 6,647,109\n",
      "Trainable params: 6,647,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IG8mXpx2bw-M"
   },
   "source": [
    "# 모델 훈련\n",
    "- 이 문제는 표준적인 분류 문제로 취급 가능  \n",
    "\n",
    "\n",
    "- 이전 RNN 상태와 현재 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측   \n",
    "\n",
    "\n",
    "- stateful RNN\n",
    "    - time step 0 의 이전 상태는 이전 batch 의 final step 을 이어 받으므로 stateful=True 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRM9M7K7OPi8"
   },
   "source": [
    "## 손실함수 지정\n",
    "\n",
    "- tf.keras.losses.sparse_categorical_crossentropy 손실 함수 이용\n",
    "\n",
    "\n",
    "    - categorical_crossentropy - one-hot-encoding  \n",
    "        [1,0,0]\n",
    "        [0,1,0]   \n",
    "        [0,0,1]  \n",
    "    - sparse_categorical_crossentropy - integer encoding\n",
    "        1  \n",
    "        2  \n",
    "        3  \n",
    "\n",
    "\n",
    "- 우리의 모델은 logit 을 출력으로 반환하므로 from_logits=True 로 setting 한다.\n",
    "    --> softmax activation 보다 계산이 안정적 (Tensorflow document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkMDGyPtOPi9"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lHaYMvCOPjL"
   },
   "source": [
    "## checkpoint 구성\n",
    "\n",
    "- 훈련 중 checkpoint 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCrhT4qiOPjM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# 체크포인트 파일 이름\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOzwYMMuxwun"
   },
   "source": [
    "### Training\n",
    "- 영문 Shakespear : 50 epoch 이상\n",
    "- 한글 어린왕자 : 300 epoch 이상 (Colab 10 분 소요)\n",
    "- 한글 이상한 나라의 앨리스 : 200 epoch 이상 (Colab 10 분 소요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "At6l7PWoOPjP",
    "outputId": "32e89273-e639-4cdd-f9f2-35ab0194569d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 0.0406 - accuracy: 0.9947\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 2s 163ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "3.241821527481079\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "history = model.fit(dataset, epochs=200, callbacks=[checkpoint_callback])\n",
    "\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "QJnwboYnxXgB",
    "outputId": "f5ebfa43-57a9-4ac8-91c3-0cf4a5a21e6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['accuracy'])\n",
    "ax1.set_title('Accuracy')\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.set_title('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRvaXsxYOPjQ"
   },
   "source": [
    "## 훈련된 model 을 이용한 text generation\n",
    "\n",
    "- 예측 단계를 간단히 하기 위해 batch size = 1 로 변경한 새로운 model 을 rebuild 하고, last checkpoint 의 model weight 를 load  \n",
    "\n",
    "\n",
    "- model rebuild 이유:\n",
    "\n",
    "    - 우리가 작성했던 model 은 stateful=True 이므로, 작성 당시에 모델이 지정된 batch_input_shape 의  fixed batch size 만 받아 들인다.\n",
    "    - 다른 batch size 에서도 수행되도록 하기 위해서는 model 을 batch_size 1 로 rebuild 하고 마지막 checkpoint 에서 저장한 model weight 를 restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "HBYVuyyFOPjS",
    "outputId": "071a78cc-3b35-4031-f0ff-d0516a2af2ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            279808    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 1093)           1120325   \n",
      "=================================================================\n",
      "Total params: 6,647,109\n",
      "Trainable params: 6,647,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# batch size 1 의 new model\n",
    "model = build_model(vocab_size=nb_chars, embedding_dim=EMBEDDING_DIM, rnn_units=RNN_UNITS, batch_size=1)\n",
    "# weight load\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# batch size 1 로 rebuild\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAXB3Jg54zP_"
   },
   "source": [
    "## 마지막 layer 에서 output sampling 요령\n",
    "\n",
    "- distribution 의 argmax 를 하면 model 이 쉽게 loop 에 빠짐.  \n",
    "\n",
    "\n",
    "- tf.random.categorical(logits, num_samples) 을 이용하여 categorical 분포로 부터 sample 추출  \n",
    "\n",
    "\n",
    "    - logits: [batch_size, num_classes] 의 2-D Tensor. 각 slice [i, :] 는 각 class 의 normalize 되지 않은 log-probability 를 나타냄\n",
    "    - num_samples: 각 row slice 에서 추출할  독립적 sample 수\n",
    "    \n",
    "- MAX_SEQ_LEN=100 이므로, 100 개의 timestep 별로 65 개의 character 별 확률([100, 65) 로  1 개를 random sampling \n",
    "  --> output shape [100, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "OoQ5TFDy4zQA",
    "outputId": "67fea6d5-febf-429d-e046-abf5c97dd693"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=int64, numpy=\n",
       "array([[35],\n",
       "       [60],\n",
       "       [13],\n",
       "       [ 8],\n",
       "       [21],\n",
       "       [28],\n",
       "       [ 5],\n",
       "       [19],\n",
       "       [12],\n",
       "       [36]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_batch_predictions = np.random.random((1, 10, 65))   # (batch_size, time_steps, num_chars)\n",
    "# 10 개 timestep 별로 (10, 65) shape 의 확률\n",
    "tf.random.categorical(ex_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40848ynCbw-v"
   },
   "source": [
    "## 예측 loop \n",
    "\n",
    "### 다음 code block 에서 text 생성:\n",
    "\n",
    "- start string 으로 시작, RNN state 초기화 및 생성할 characters 수 설정 \n",
    "\n",
    "- start string 과 RNN state 를 이용하여 next character 의 prediction 분포를 가져옴  \n",
    "\n",
    "- categorical 분포를 이용하여 예측된 character 의 index 계산. 예측된 character 를 model 의 next input 으로 사용  \n",
    "\n",
    "- model 에서 반환된 RNN state 는 model 로 다시 되돌려져서 이제는 하나의 character 가 아닌 더 많은 문맥(context) 에 더해짐\n",
    "\n",
    "- 다음 단어를 예측한 후 수정된 RNN state 가 다시 모델로 피드백되어 더 많은 context 를 얻으며 학습되는 방식임\n",
    "\n",
    "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nqzE9cDkOPjV",
    "outputId": "ee5c1234-2c09-45cc-e89c-e4c9202a8b01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[696, 435, 640, 281,   1, 708, 286,   1, 733, 720]], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start_string = \"ROMEO: \"\n",
    "#start_string = \"여섯 살 적에\"\n",
    "start_string = \"앨리스는 언니 옆에\"\n",
    "\n",
    "num_generate = 1000   # 생성할 문자의 수\n",
    "\n",
    "# starting_string 의 숫자화 (벡터화)\n",
    "input_eval = [char2idx[c] for c in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "input_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChRODyjscLQQ"
   },
   "source": [
    "### stateful RNN 이므로 새로운 text generation 을 위해 reset_states() method 를 이용하여 initial state 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3rWfdRX9OPjY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_generated = []\n",
    "\n",
    "model.reset_states()   # initial state\n",
    "\n",
    "for i in range(num_generate):\n",
    "    predictions = model(input_eval)    \n",
    "    # batch dimension 제거\n",
    "    predictions = tf.squeeze(predictions, 0)  \n",
    "    # greedy(argmax) 하지 않도록 분포에서 sampling\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() # scalar\n",
    "    \n",
    "    #print(predictions.shape)\n",
    "    # predicted = tf.random.categorical(predictions, num_samples=1)\n",
    "    # print(predicted)\n",
    "    # print(predicted[-1, 0])\n",
    "    # break\n",
    "    \n",
    "    # 예측된 character 를 이전 은닉 상태와 함께 다음 입력으로 model 에 전달\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)   \n",
    "    text_generated.append(idx2char[predicted_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "8efPr3DmOPja",
    "outputId": "eb694add-dbc1-4203-b929-bc815909110a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앨리스는 언니 옆에 앉아 할일 없이 고양이야 말을 치기 시작했어요. “다들 사람들도 하나도 없이,”라며 겨울잠쥐가 말했어요. “…우물 안에 살았더랬지요?”\n",
      "“자넨 정말 날 하신다니까,”라며 하인이 대꾸했어요. “여기 앉아 있을 게다…”\n",
      "그 순간 집 문이 벌컥 열리더니 큰 접시가 날아오더니 하인은 어째 밖에 있던 거예요.\n",
      "그녀는 자신이 좀 전에 했던 말을 되풀이할 뿐이었죠.\n",
      "“그건 모두 그의 상상이야, 슬픈 건 실제야 해,”라며 하인의 경기한 계속 말했어요.\n",
      "“…그럼 바닷가재의 카드리유(사교댄스 이름)에 대한 비유나 계속 해볼까?”라며 그리핀이 말했어요. “그렇지 않음 네가 겁을 먹든 말든 사형에 처하겠노라.” \n",
      "“제발요, 폐하, 전 그냥 가난한 무척 커졌기에 이제 왕의 말에 끼어드는 것도 조금도 두렵지가 않았어요. “당사자들 끼리 1만원(원문→6펜스)을 집어넣고 가려고 킥렸답니다.\n",
      "그래서 까지 고개를 흔들어보였어요. “무슨 꿍꿍이속이 없다고 말씀하시겠는데, 아마 저들도 자주 일을 수 있었답니다. 밤 명 낮은 문 옆에서 있다는 거니?”\n",
      "“오, 대구라.”라며 가짜 거북이가 말했어요. “쇠를 들어왔어요.\n",
      "“포도주 좀 마시련.”라며 3월 토끼가 권하며 말했어요.\n",
      "애릴스가 테이블을 다 둘러보았지만 거긴 차말고는 어떤 다른 것도 없었어요.\n",
      "“포도주가 없는데요.”라며 앨리스가 한마디 살짝 비명을 지르더니 이내 곧 얘기를 계속 이어갔어요.\n",
      "“…우선 ‘ㅁ’으로 시작되는, ‘마’우스트랩(신도 있다는 발음 좋아.’라며 앨리스가 곧 제를 내며 말했어요. “그러니 위에 편지는 사람들곤 고슴도치와 지붕(수프를 기우하는지 살려고 그녀가 다시 생각했지요.\n",
      "“이렇게, 어. 나 좀 더 잘 이해할 수 있어. 이제 가보자,”라며 앨리스가 한마디 살짝 비명을 지르더니 이내 곧 얘기를 계속 이어갔어요.\n",
      "“…우선 ‘ㅁ’으로 시작되는, ‘마’우스트랩레덫)들 중 다과 증인없는 대구였거든요.”라고 토끼가 말하기에, 앨리스로선 웃음보가 터지지 않을 수 없었어요.\n",
      "“오, 쉿!”라며 토끼가 경악해선 속삭였어요. “여왕폐하께서 놀\n"
     ]
    }
   ],
   "source": [
    "print(start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WInCg4up90J3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "065_Generating_Text_Next_Character_Shakespeare.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
