{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33f0501",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "# CLIP과의 상호작용\n",
    "\n",
    "CLIP 모델을 다운로드 및 실행하고, 임의 이미지와 텍스트 입력 간의 유사성을 계산하고, 제로샷 이미지 분류를 수행하는 방법을 보여줍니다.\n",
    "\n",
    "### Colab 에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub에서 openai의 CLIP 리포지토리를 직접 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3e561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f2df783",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# 모델 로딩\n",
    "\n",
    "`clip.available_models()`는 사용 가능한 CLIP 모델의 이름을 나열합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f9875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c8173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 모델 및 전처리 함수를 로드합니다. \"ViT-B/32\"는 CLIP의 모델 아키텍처를 지정합니다.\n",
    "# 모델을 GPU로 옮기고 평가 모드로 설정합니다.\n",
    "# 모델의 입력 해상도, 문맥 길이, 어휘 크기 정보를 가져옵니다.\n",
    "# 모델의 파라미터 수, 입력 해상도, 문맥 길이, 어휘 크기를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d1b58",
   "metadata": {
    "id": "21slhZGCqANb"
   },
   "source": [
    "# 이미지 전처리\n",
    "\n",
    "모델이 기대하는 이미지 해상도에 맞게 입력 이미지의 크기를 조정하고 가운데 부분을 자릅니다. 그 전에 데이터 세트 평균과 표준 편차를 사용하여 픽셀 강도를 정규화합니다.\n",
    "\n",
    "`clip.load()`의 두 번째 반환 값에는 이 사전 처리를 수행하는 torchvision `Transform`이 포함되어 있습니다.\n",
    "\n",
    "    - Resize - 이미지를 크기 224로 리사이즈합니다.   \n",
    "    - CenterCrop - 리사이즈된 이미지의 중심을 기준으로 224 x 224 크기로 자르기를 수행  \n",
    "    - _convert_image_to_rgb - 이미지를 RGB 형식으로 변환  \n",
    "    - ToTensor - 이미지를 **텐서(tensor)**로 변환. 이때 픽셀 값은 [0, 255] 범위에서 [0, 1] 범위로 정규화.  \n",
    "    - Normalize - 각 채널(R, G, B)에 대해 **평균(mean)**과 **표준편차(std)**를 사용하여 픽셀 값의 범위를 조정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34e20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dee5c1e",
   "metadata": {
    "id": "xwSB5jZki3Cj"
   },
   "source": [
    "# 텍스트 전처리\n",
    "\n",
    "`clip.tokenize()`를 사용하여 대소문자를 구분하지 않는 토크나이저를 사용합니다. 기본적으로 출력은 77개의 토큰 길이가 되도록 채워지며, 이는 CLIP 모델이 예상하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf47020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ba8454",
   "metadata": {
    "id": "4W8ARJVqBJXs"
   },
   "source": [
    "# 입력 이미지 및 텍스트 설정\n",
    "\n",
    "8개의 예시 이미지와 해당 텍스트 설명을 모델에 제공하고 해당 feature 간의 유사성을 비교할 것입니다.\n",
    "\n",
    "토크나이저는 대소문자를 구분하지 않으며 적절한 텍스트 설명을 자유롭게 제공할 수 있습니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/HmfDS6g.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74603da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook 환경에서 시각화한 그래프나 이미지가 더욱 선명하게 출력되도록 하는 설정\n",
    "# skimage 내의 이미지들을 prompt engineering 하여 저장 (파일명: 파일명을 포함한 문장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95b4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지와 처리된 이미지, 그리고 텍스트를 저장하기 위한 리스트를 초기화합니다.\n",
    "# skimage의 데이터 디렉토리에서 .png 또는 .jpg로 끝나는 모든 파일을 반복하여 처리합니다.\n",
    "    # 파일명에서 확장자를 제거하여 이미지의 이름을 얻습니다.\n",
    "    # 설명이 없는 이미지는 무시합니다.\n",
    "    # 이미지를 열어 RGB 포맷으로 변환합니다.\n",
    "    # 현재 처리중인 이미지를 서브플롯으로 추가하고, 제목과 함께 그림을 그립니다.\n",
    "    # 원본 이미지, 전처리된 이미지, 그리고 해당 이미지의 설명을 각각의 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f5447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbc135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81195f7",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## feature 생성\n",
    "\n",
    "이미지를 벡터화하고, 각 텍스트 입력을 토큰화하고, 모델의 순방향 전파를 실행하여 이미지와 텍스트 feature를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 리스트를 numpy 배열로 변환한 후, 이를 파이토치 텐서로 변환후 GPU로 전송\n",
    "# 이미지에 대한 설명들 앞에 \"This is \"를 추가한 후, CLIP의 tokenize 함수를 사용하여 텍스트 토큰을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67e9a5",
   "metadata": {
    "id": "l-hd5fS4AmJC"
   },
   "source": [
    "토큰화가 잘 되었는지 원래의 text 를 복원하여 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP에서 사용하는 토크나이저 로드\n",
    "# text_tokens의 첫 번째 항목을 detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 이미지 입력을 모델에 전달하여 이미지 특성을 추출\n",
    "    # 텍스트 토큰을 모델에 전달하여 텍스트 특성을 추출\n",
    "# 코사인 유사도 계산을 위해 8개의 image feature shape 과 text feature shape 이 동일한지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109e59c",
   "metadata": {
    "id": "cuxm2Gt4Wvzt"
   },
   "source": [
    "## 코사인 유사도 계산\n",
    "\n",
    "feature를 정규화하고 각 쌍의 내적을 계산합니다.  \n",
    "특성 vetor를 정규화 하면 길이가 1 이 되므로 벡터의 크기를 신경 쓰지 않고 방향 정보만 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지/text 특성의 L2 Norm(유클리드 노름)을 계산하여 특성 벡터를 정규화합니다.\n",
    "# 텍스트 특성과 이미지 특성 사이의 유사성을 계산하기 위해 내적(dot product) 연산을 수행합니다.\n",
    "# 결과적으로, 이 유사성 행렬의 각 요소는 특정 텍스트 설명과 이미지 간의 유사성 점수를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55615eb",
   "metadata": {
    "id": "nMV_uhwx-X3k"
   },
   "source": [
    "사전 학습된 CLIP 모델이 image feature 와 text feature 간의 유사도를 정확히 반영하는지 시각화하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 제목 설정\n",
    "# 코사인 유사도 매트릭스를 색상으로 시각화\n",
    "# 유사도 매트릭스의 각 셀에 해당하는 값을 표시\n",
    "# x축과 y축의 범위 설정\n",
    "# 이미지를 유사도 매트릭스 위로 표시\n",
    "    # 플롯의 특정 부분에 이미지를 배치\n",
    "# 그래프 주변의 테두리 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ab032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
